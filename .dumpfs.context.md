<codebase name="dumpfs">

================================================
/home/user/projs/dumpfs/src/git/error.rs
================================================

```rs
/*!
 * Error types for Git operations
 */

use indicatif::style::TemplateError;
use thiserror::Error;

/// Errors that can occur during Git operations
#[derive(Error, Debug)]
pub enum GitError {
    /// Invalid Git URL format
    #[error("Invalid Git URL: {0}")]
    InvalidUrl(String),

    /// Error opening a Git repository
    #[error("Failed to open repository: {0}")]
    OpenError(git2::Error),

    /// Error cloning a Git repository
    #[error("Failed to clone repository: {0}")]
    CloneError(git2::Error),

    /// Error fetching from remote
    #[error("Failed to fetch from remote: {0}")]
    FetchError(git2::Error),

    /// Git2 error (generic)
    #[error("Git error: {0}")]
    Git2Error(#[from] git2::Error),

    /// IO error during Git operations
    #[error("IO error: {0}")]
    IoError(#[from] std::io::Error),

    /// Repository not found
    #[error("Repository not found: {0}")]
    NotFound(String),

    /// Progress Error
    #[error("Progress Template failed to be parssed: {0}")]
    TemplateError(#[from] TemplateError),
}

/// Specialized Result type for Git operations
pub type GitResult<T> = Result<T, GitError>;

```

================================================
/home/user/projs/dumpfs/src/git/progress.rs
================================================

```rs
/*!
 * Progress reporting for Git operations
 */

use indicatif::ProgressBar;

use crate::utils::format_file_size;

use super::GitRepoInfo;

/// Trait for reporting Git operation progress
pub trait ProgressReporter {
    /// Called with progress information during Git operations
    fn report(&self, progress: &GitProgress);
}

/// Progress information for Git operations
#[derive(Debug, Clone)]
pub struct GitProgress {
    /// Total number of objects to download
    pub total_objects: usize,
    /// Number of received objects
    pub received_objects: usize,
    /// Number of indexed objects
    pub indexed_objects: usize,
    /// Number of local objects
    pub local_objects: usize,
    /// Total number of deltas
    pub total_deltas: usize,
    /// Number of indexed deltas
    pub indexed_deltas: usize,
    /// Number of bytes received
    pub received_bytes: usize,
}

impl GitProgress {
    /// Get the progress percentage
    pub fn percentage(&self) -> u8 {
        if self.total_objects == 0 {
            return 0;
        }

        ((self.received_objects * 100) / self.total_objects) as u8
    }

    /// Get a formatted string of received bytes
    pub fn formatted_bytes(&self) -> String {
        format_file_size(self.received_bytes as u64)
    }
}

// Implement ProgressReporter for closures
impl<F> ProgressReporter for F
where
    F: Fn(&GitProgress),
{
    fn report(&self, progress: &GitProgress) {
        self(progress)
    }
}

pub struct ProgressBarAdapter<'a> {
    pub bar: &'a ProgressBar,
    pub repo_info: &'a GitRepoInfo,
    pub is_clone: bool,
}

impl ProgressReporter for ProgressBarAdapter<'_> {
    fn report(&self, git_progress: &GitProgress) {
        let percent = git_progress.percentage();

        // Format progress message
        let action = if self.is_clone { "Cloning" } else { "Updating" };
        let msg = format!(
            "{} {}/{} ({}/{}), {} downloaded",
            action,
            self.repo_info.owner,
            self.repo_info.name,
            git_progress.received_objects,
            git_progress.total_objects,
            git_progress.formatted_bytes()
        );

        self.bar.set_message(msg);
        self.bar.set_position(percent as u64);
    }
}

```

================================================
/home/user/projs/dumpfs/src/git/url.rs
================================================

```rs
/*!
 * Git URL parsing and handling
 */

use std::path::PathBuf;
use std::str::FromStr;

use once_cell::sync::Lazy;
use regex::Regex;
use url::Url;

use super::error::{GitError, GitResult};

// Statically compiled regexes for better performance
static HTTP_REGEX: Lazy<Regex> = Lazy::new(|| {
    Regex::new(
        r"^https?://(?:www\.)?(?:github\.com|gitlab\.com|bitbucket\.org|.*)/[^/]+/[^/]+(?:\.git)?$",
    )
    .expect("HTTP URL regex pattern should be valid")
});

static SSH_REGEX: Lazy<Regex> = Lazy::new(|| {
    Regex::new(r"^git@(?:github\.com|gitlab\.com|bitbucket\.org|[^:]+):[^/]+/[^/]+(?:\.git)?$")
        .expect("SSH URL regex pattern should be valid")
});

static SSH_PARSE_REGEX: Lazy<Regex> = Lazy::new(|| {
    Regex::new(r"^git@([^:]+):([^/]+)/([^/]+)(?:\.git)?$")
        .expect("SSH parse regex pattern should be valid")
});

/// Git hosting platform types
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum GitHost {
    /// GitHub repository
    GitHub,
    /// GitLab repository
    GitLab,
    /// Bitbucket repository
    Bitbucket,
    /// Other Git hosting
    Other(String),
}

impl std::fmt::Display for GitHost {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            GitHost::GitHub => write!(f, "GitHub"),
            GitHost::GitLab => write!(f, "GitLab"),
            GitHost::Bitbucket => write!(f, "Bitbucket"),
            GitHost::Other(host) => write!(f, "{}", host),
        }
    }
}

/// Information about a Git repository
#[derive(Debug, Clone)]
pub struct GitRepoInfo {
    /// Original URL
    pub url: String,
    /// Git hosting platform
    pub host: GitHost,
    /// Repository owner/username
    pub owner: String,
    /// Repository name
    pub name: String,
    /// Local cache path
    pub cache_path: PathBuf,
}

impl std::fmt::Display for GitRepoInfo {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}/{}/{}", self.host, self.owner, self.name)
    }
}

impl FromStr for GitRepoInfo {
    type Err = GitError;

    fn from_str(url: &str) -> Result<Self, Self::Err> {
        // Check if the URL is valid
        if !HTTP_REGEX.is_match(url) && !SSH_REGEX.is_match(url) {
            return Err(GitError::InvalidUrl(url.to_string()));
        }

        // Handle HTTP/HTTPS URLs
        if url.starts_with("http://") || url.starts_with("https://") {
            if let Ok(parsed_url) = Url::parse(url) {
                let host_str = parsed_url
                    .host_str()
                    .ok_or_else(|| GitError::InvalidUrl(format!("Invalid host in URL: {}", url)))?;

                // Get path without leading slash
                let path = parsed_url.path();
                let path = path.strip_prefix('/').unwrap_or(path); // Safe unwrap since we're providing a fallback

                let path_segments: Vec<&str> = path.split('/').collect();

                if path_segments.len() < 2 {
                    return Err(GitError::InvalidUrl(format!(
                        "Missing owner or repository in URL: {}",
                        url
                    )));
                }

                let owner = path_segments[0].to_string();
                let mut name = path_segments[1].to_string();

                // Remove .git suffix if present
                if name.ends_with(".git") {
                    name = name[0..name.len() - 4].to_string();
                }

                let host = match host_str {
                    "github.com" => GitHost::GitHub,
                    "gitlab.com" => GitHost::GitLab,
                    "bitbucket.org" => GitHost::Bitbucket,
                    _ => GitHost::Other(host_str.to_string()),
                };

                let cache_path = get_cache_path(&host, &owner, &name);

                return Ok(GitRepoInfo {
                    url: url.to_string(),
                    host,
                    owner,
                    name,
                    cache_path,
                });
            }
        }

        // Handle SSH URLs (git@github.com:owner/repo.git)
        if url.starts_with("git@") {
            if let Some(captures) = SSH_PARSE_REGEX.captures(url) {
                if let (Some(host_match), Some(owner_match), Some(name_match)) =
                    (captures.get(1), captures.get(2), captures.get(3))
                {
                    let host_str = host_match.as_str();
                    let owner = owner_match.as_str().to_string();
                    let mut name = name_match.as_str().to_string();

                    // Remove .git suffix if present
                    if name.ends_with(".git") {
                        name = name[0..name.len() - 4].to_string();
                    }

                    let host = match host_str {
                        "github.com" => GitHost::GitHub,
                        "gitlab.com" => GitHost::GitLab,
                        "bitbucket.org" => GitHost::Bitbucket,
                        _ => GitHost::Other(host_str.to_string()),
                    };

                    let cache_path = get_cache_path(&host, &owner, &name);

                    return Ok(GitRepoInfo {
                        url: url.to_string(),
                        host,
                        owner,
                        name,
                        cache_path,
                    });
                }
            }
        }

        Err(GitError::InvalidUrl(url.to_string()))
    }
}

/// Check if a path is a Git repository URL
pub fn is_git_url(path: &str) -> bool {
    path.parse::<GitRepoInfo>().is_ok()
}

/// Parse a Git repository URL into components
pub fn parse_git_url(url: &str) -> GitResult<GitRepoInfo> {
    url.parse()
}

/// Get the cache directory path for a repository
pub fn get_cache_path(host: &GitHost, owner: &str, name: &str) -> PathBuf {
    let mut cache_dir = dirs::cache_dir().unwrap_or_else(|| PathBuf::from("~/.cache")); // Safe unwrap with fallback
    cache_dir = cache_dir.join("dumpfs");

    match host {
        GitHost::GitHub => cache_dir.join("github").join(owner).join(name),
        GitHost::GitLab => cache_dir.join("gitlab").join(owner).join(name),
        GitHost::Bitbucket => cache_dir.join("bitbucket").join(owner).join(name),
        GitHost::Other(host_name) => cache_dir.join("git").join(host_name).join(owner).join(name),
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_is_git_url() {
        // Test GitHub URLs
        assert!(is_git_url(&"https://github.com/username/repo".to_string()));
        assert!(is_git_url(
            &"https://github.com/username/repo.git".to_string()
        ));
        assert!(is_git_url(&"git@github.com:username/repo.git".to_string()));

        // Test GitLab URLs
        assert!(is_git_url(&"https://gitlab.com/username/repo".to_string()));
        assert!(is_git_url(
            &"https://gitlab.com/username/repo.git".to_string()
        ));
        assert!(is_git_url(&"git@gitlab.com:username/repo.git".to_string()));

        // Test Bitbucket URLs
        assert!(is_git_url(
            &"https://bitbucket.org/username/repo".to_string()
        ));
        assert!(is_git_url(
            &"https://bitbucket.org/username/repo.git".to_string()
        ));
        assert!(is_git_url(
            &"git@bitbucket.org:username/repo.git".to_string()
        ));

        // Test custom Git host URLs
        assert!(is_git_url(
            &"https://git.example.com/username/repo".to_string()
        ));
        assert!(is_git_url(
            &"https://git.example.com/username/repo.git".to_string()
        ));
        assert!(is_git_url(
            &"git@git.example.com:username/repo.git".to_string()
        ));

        // Test invalid URLs
        assert!(!is_git_url(&"https://github.com".to_string()));
        assert!(!is_git_url(&"https://github.com/username".to_string()));
        assert!(!is_git_url(&"git@github.com".to_string()));
        assert!(!is_git_url(&"/path/to/local/directory".to_string()));
        assert!(!is_git_url(&"username/repo".to_string()));
    }

    #[test]
    fn test_parse_git_url() {
        // Test GitHub HTTPS URL
        let repo = parse_git_url(&"https://github.com/username/repo".to_string()).unwrap();
        assert_eq!(repo.url, "https://github.com/username/repo");
        assert!(matches!(repo.host, GitHost::GitHub));
        assert_eq!(repo.owner, "username");
        assert_eq!(repo.name, "repo");

        // Test GitHub SSH URL
        let repo = parse_git_url(&"git@github.com:username/repo.git".to_string()).unwrap();
        assert_eq!(repo.url, "git@github.com:username/repo.git");
        assert!(matches!(repo.host, GitHost::GitHub));
        assert_eq!(repo.owner, "username");
        assert_eq!(repo.name, "repo");

        // Test custom host cache path
        let host = GitHost::Other("example.com".to_string());
        let owner = "username";
        let name = "repo";
        let cache_path = get_cache_path(&host, owner, name);
        assert!(cache_path.ends_with(
            &std::path::Path::new("git")
                .join("example.com")
                .join("username")
                .join("repo")
        ));
    }
}

```

================================================
/home/user/projs/dumpfs/src/git/cache.rs
================================================

```rs
/*!
 * Git repository cache management
 */

use std::fs;
use std::io;
use std::path::{Path, PathBuf};
use std::time::{Duration, SystemTime};

/// Clean up old repositories from cache
pub fn clean_cache(days: u64) -> io::Result<usize> {
    let cache_dir = dirs::cache_dir()
        .unwrap_or_else(|| PathBuf::from("~/.cache"))
        .join("dumpfs");

    if !cache_dir.exists() {
        return Ok(0);
    }

    let now = SystemTime::now();
    let max_age = Duration::from_secs(days * 24 * 60 * 60);

    // Clean all provider directories
    let providers = ["github", "gitlab", "bitbucket", "git"];

    providers
        .iter()
        .map(|provider| cache_dir.join(provider))
        .filter(|path| path.exists())
        .try_fold(0, |acc, path| {
            let count = clean_cache_dir(&path, &max_age, &now)?;
            Ok(acc + count)
        })
}

/// Clean up repositories in a specific cache directory
fn clean_cache_dir(dir: &Path, max_age: &Duration, now: &SystemTime) -> io::Result<usize> {
    if !dir.exists() {
        return Ok(0);
    }

    let mut count = 0;

    for entry in fs::read_dir(dir)? {
        let entry = entry?;
        let path = entry.path();

        if !path.is_dir() {
            continue;
        }

        if path.join(".git").exists() {
            // It's a repository, check age
            if let Ok(metadata) = fs::metadata(&path) {
                if let Ok(modified) = metadata.modified() {
                    if let Ok(age) = now.duration_since(modified) {
                        if age > *max_age {
                            // Remove old repository
                            fs::remove_dir_all(&path)?;
                            count += 1;
                        }
                    }
                }
            }
        } else {
            // It's a directory structure (like owner), recurse
            count += clean_cache_dir(&path, max_age, now)?;
        }
    }

    Ok(count)
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::env;
    use std::fs::File;
    use std::io::Write;
    use tempfile::tempdir;

    #[test]
    fn test_clean_cache() -> io::Result<()> {
        // Create a temporary directory for testing
        let temp_dir = tempdir()?;
        let cache_dir = temp_dir.path().join("dumpfs");

        // Create structure for a GitHub repo
        let repo_path = cache_dir.join("github").join("username").join("repo");
        fs::create_dir_all(&repo_path)?;

        // Create a .git directory to identify it as a repo
        fs::create_dir_all(repo_path.join(".git"))?;

        // Create a file with old modification time
        let file_path = repo_path.join("test.txt");
        let mut file = File::create(&file_path)?;
        writeln!(file, "Test content")?;

        // Override cache dir location for testing
        let original_cache_dir = env::var("XDG_CACHE_HOME").ok();
        env::set_var("XDG_CACHE_HOME", temp_dir.path());

        // Call clean_cache_dir directly with zero days (should clean everything)
        let now = SystemTime::now();
        let max_age = Duration::from_secs(0); // 0 days means everything is older
        let cleaned = clean_cache_dir(&cache_dir.join("github"), &max_age, &now)?;

        assert_eq!(cleaned, 1); // Should clean up our one repo

        // Restore original env var
        if let Some(original) = original_cache_dir {
            env::set_var("XDG_CACHE_HOME", original);
        } else {
            env::remove_var("XDG_CACHE_HOME");
        }

        Ok(())
    }
}

```

================================================
/home/user/projs/dumpfs/src/git/repository.rs
================================================

```rs
/*!
 * Git repository operations
 */

use std::fs;
use std::path::PathBuf;

use git2::{FetchOptions, RemoteCallbacks, Repository as Git2Repository};

use super::error::{GitError, GitResult};
use super::progress::{GitProgress, ProgressReporter};
use super::url::GitRepoInfo;

/// Git repository with associated information
pub struct Repository {
    /// Inner git2 repository instance
    inner: Git2Repository,
    /// Repository information
    info: GitRepoInfo,
}

impl Repository {
    /// Open an existing Git repository
    pub fn open(info: GitRepoInfo) -> GitResult<Self> {
        let repo = Git2Repository::open(&info.cache_path).map_err(GitError::OpenError)?;

        Ok(Self { inner: repo, info })
    }

    /// Check if a repository exists at the given cache path
    pub fn exists(info: &GitRepoInfo) -> bool {
        info.cache_path.join(".git").exists()
    }

    /// Clone a Git repository
    pub fn clone<P: ProgressReporter>(info: GitRepoInfo, progress: Option<&P>) -> GitResult<Self> {
        // Create cache directory if it doesn't exist
        fs::create_dir_all(&info.cache_path).map_err(GitError::IoError)?;

        // Setup builder with progress reporting
        let mut builder = git2::build::RepoBuilder::new();

        // Configure fetch options with zero depth for shallow clone
        let mut fetch_options = FetchOptions::new();
        fetch_options.download_tags(git2::AutotagOption::None);
        fetch_options.depth(1);

        if let Some(reporter) = progress {
            let mut callbacks = RemoteCallbacks::new();
            callbacks.transfer_progress(|stats| {
                let progress = GitProgress {
                    total_objects: stats.total_objects(),
                    received_objects: stats.received_objects(),
                    indexed_objects: stats.indexed_objects(),
                    local_objects: stats.local_objects(),
                    total_deltas: stats.total_deltas(),
                    indexed_deltas: stats.indexed_deltas(),
                    received_bytes: stats.received_bytes(),
                };
                reporter.report(&progress);
                true
            });

            fetch_options.remote_callbacks(callbacks);
        }

        builder.fetch_options(fetch_options);

        // Clone the repository
        let repo = builder
            .clone(&info.url, &info.cache_path)
            .map_err(GitError::CloneError)?;

        Ok(Self { inner: repo, info })
    }

    /// Pull latest changes for an existing repository
    pub fn pull<P: ProgressReporter>(&mut self, progress: Option<&P>) -> GitResult<()> {
        // Set up fetch options with progress reporting
        let mut fetch_options = FetchOptions::new();
        fetch_options.download_tags(git2::AutotagOption::None);
        fetch_options.depth(0); // Set fetch depth to zero for minimal history

        if let Some(reporter) = progress {
            let mut callbacks = RemoteCallbacks::new();
            callbacks.transfer_progress(|stats| {
                let progress = GitProgress {
                    total_objects: stats.total_objects(),
                    received_objects: stats.received_objects(),
                    indexed_objects: stats.indexed_objects(),
                    local_objects: stats.local_objects(),
                    total_deltas: stats.total_deltas(),
                    indexed_deltas: stats.indexed_deltas(),
                    received_bytes: stats.received_bytes(),
                };
                reporter.report(&progress);
                true
            });

            fetch_options.remote_callbacks(callbacks);
        }

        // Fetch from remote
        let mut remote = self
            .inner
            .find_remote("origin")
            .map_err(GitError::FetchError)?;

        remote
            .fetch(&["main", "master"], Some(&mut fetch_options), None)
            .map_err(GitError::FetchError)?;

        // Find remote branch to reset to
        let remote_branch = self
            .inner
            .find_reference("refs/remotes/origin/master")
            .or_else(|_| self.inner.find_reference("refs/remotes/origin/main"))
            .map_err(GitError::FetchError)?;

        // Get object to reset to
        let branch_name = remote_branch.name().ok_or_else(|| {
            GitError::FetchError(git2::Error::from_str("Failed to get branch name"))
        })?;

        let obj = self
            .inner
            .revparse_single(branch_name)
            .map_err(GitError::FetchError)?;

        // Reset to remote branch
        self.inner
            .reset(&obj, git2::ResetType::Hard, None)
            .map_err(GitError::FetchError)?;

        Ok(())
    }

    /// Get repository information
    pub fn info(&self) -> &GitRepoInfo {
        &self.info
    }

    /// Get path to the repository
    pub fn path(&self) -> &PathBuf {
        &self.info.cache_path
    }
}

/// Repository operation builder for more flexible configuration
pub struct RepositoryBuilder {
    /// Repository information
    info: GitRepoInfo,
    /// Optional fetch options
    fetch_options: Option<FetchOptions<'static>>,
}

impl RepositoryBuilder {
    /// Create a new repository builder
    pub fn new(info: GitRepoInfo) -> Self {
        Self {
            info,
            fetch_options: None,
        }
    }

    /// Configure with progress reporting
    pub fn with_progress<P: ProgressReporter + 'static>(mut self, reporter: P) -> Self {
        let mut callbacks = RemoteCallbacks::new();
        callbacks.transfer_progress(move |stats| {
            let progress = GitProgress {
                total_objects: stats.total_objects(),
                received_objects: stats.received_objects(),
                indexed_objects: stats.indexed_objects(),
                local_objects: stats.local_objects(),
                total_deltas: stats.total_deltas(),
                indexed_deltas: stats.indexed_deltas(),
                received_bytes: stats.received_bytes(),
            };
            reporter.report(&progress);
            true
        });

        let mut fetch_options = FetchOptions::new();
        fetch_options.download_tags(git2::AutotagOption::None);
        fetch_options.depth(0); // Set fetch depth to zero for minimal history
        fetch_options.remote_callbacks(callbacks);
        self.fetch_options = Some(fetch_options);

        self
    }

    /// Clone the repository
    pub fn clone(self) -> GitResult<Repository> {
        // Create cache directory if it doesn't exist
        fs::create_dir_all(&self.info.cache_path).map_err(GitError::IoError)?;

        // Setup builder
        let mut builder = git2::build::RepoBuilder::new();

        if let Some(fetch_options) = self.fetch_options {
            builder.fetch_options(fetch_options);
        }

        // Clone the repository
        let repo = builder
            .clone(&self.info.url, &self.info.cache_path)
            .map_err(GitError::CloneError)?;

        Ok(Repository {
            inner: repo,
            info: self.info,
        })
    }

    /// Open an existing repository
    pub fn open(self) -> GitResult<Repository> {
        Repository::open(self.info)
    }
}

```

================================================
/home/user/projs/dumpfs/src/git/mod.rs
================================================

```rs
/*!
 * Git repository handling functionality
 */

mod cache;
mod error;
mod progress;
mod repository;
mod url;

// Re-export public items
pub use cache::clean_cache;
use clap::ValueEnum;
pub use error::{GitError, GitResult};
use indicatif::{ProgressBar, ProgressStyle};
use progress::ProgressBarAdapter;
pub use progress::{GitProgress, ProgressReporter};
pub use repository::{Repository, RepositoryBuilder};
pub use url::{is_git_url, parse_git_url, GitHost, GitRepoInfo};

use std::io;
use std::path::PathBuf;

/// Policy for handling Git repository caching
#[derive(Debug, Clone, Copy, PartialEq, Eq, ValueEnum)]
pub enum GitCachePolicy {
    /// Always pull latest changes for existing repositories (default)
    AlwaysPull,
    /// Delete and re-clone existing repositories
    ForceClone,
    /// Use cached repositories without pulling updates
    UseCache,
}

impl Default for GitCachePolicy {
    fn default() -> Self {
        Self::AlwaysPull
    }
}

/// Clone or update a Git repository
///
/// This function maintains compatibility with the original API
/// while using the new implementation internally.
pub fn clone_repository<P: ProgressReporter>(
    url: &str,
    progress_fn: Option<&P>,
) -> io::Result<PathBuf> {
    // Parse the URL
    let info = match url::parse_git_url(url) {
        Ok(info) => info,
        Err(e) => return Err(io::Error::new(io::ErrorKind::InvalidInput, e.to_string())),
    };

    // Check if repository already exists
    if Repository::exists(&info) {
        // Try to open and pull
        match Repository::open(info.clone()) {
            Ok(mut repo) => {
                if let Err(e) = repo.pull(progress_fn) {
                    return Err(io::Error::new(io::ErrorKind::Other, e.to_string()));
                }
                Ok(repo.path().clone())
            }
            Err(e) => Err(io::Error::new(io::ErrorKind::Other, e.to_string())),
        }
    } else {
        // Clone the repository
        match Repository::clone(info.clone(), progress_fn) {
            Ok(repo) => Ok(repo.path().clone()),
            Err(e) => Err(io::Error::new(io::ErrorKind::Other, e.to_string())),
        }
    }
}

// Create a progress reporter adapter
pub fn process_path(
    path: &str,
    git_cache_policy: GitCachePolicy,
    progress: Option<&ProgressBar>,
) -> GitResult<(PathBuf, Option<String>, Option<GitRepoInfo>)> {
    // If not a Git URL, just return the path as is
    if !is_git_url(path) {
        return Ok((PathBuf::from(path), None, None));
    }

    // Parse the Git URL
    let repo_info = parse_git_url(path)?;

    // Use the provided progress bar or create a new one
    let progress_bar = match progress {
        Some(p) => p,
        None => {
            // Create a new progress bar if none is provided
            let new_bar = ProgressBar::new(100);
            new_bar.set_style(ProgressStyle::default_bar().template(
                "{spinner:.green} {prefix:.bold.cyan} {msg} [{bar:40.cyan/blue}] {percent}%",
            )?);

            // Since this is a temporary variable, we'll just leak it to avoid ownership issues
            Box::leak(Box::new(new_bar))
        }
    };

    // Check if repository already exists
    let repo_exists = Repository::exists(&repo_info);

    // Handle based on policy
    match (git_cache_policy, repo_exists) {
        // Repository doesn't exist, always clone
        (_, false) => {
            progress_bar.set_prefix("ðŸ”„ Cloning");
            progress_bar.set_message(format!(
                "Cloning repository: {}/{}",
                repo_info.owner, repo_info.name
            ));

            let reporter = ProgressBarAdapter {
                bar: progress_bar,
                repo_info: &repo_info,
                is_clone: true,
            };

            let repo = Repository::clone(repo_info.clone(), Some(&reporter))
                .inspect(|_| {
                    progress_bar.finish_with_message(format!(
                        "Repository cloned: {}/{}",
                        repo_info.owner, repo_info.name
                    ));
                })
                .inspect_err(|e| {
                    progress_bar.abandon_with_message(format!("Failed to clone repository: {}", e));
                })?;

            Ok((repo.path().clone(), Some(path.to_string()), Some(repo_info)))
        }

        // Force clone even if exists
        (GitCachePolicy::ForceClone, true) => {
            // Delete existing repo
            progress_bar.set_prefix("ðŸ—‘ï¸ Removing");
            progress_bar.set_message(format!(
                "Removing existing repository: {}/{}",
                repo_info.owner, repo_info.name
            ));

            // Remove the directory to force a fresh clone
            std::fs::remove_dir_all(&repo_info.cache_path).inspect_err(|e| {
                progress_bar
                    .abandon_with_message(format!("Failed to remove existing repository: {}", e));
            })?;

            // Clone the repository
            progress_bar.set_prefix("ðŸ”„ Cloning");
            progress_bar.set_message(format!(
                "Cloning repository: {}/{}",
                repo_info.owner, repo_info.name
            ));

            let reporter = ProgressBarAdapter {
                bar: progress_bar,
                repo_info: &repo_info,
                is_clone: true,
            };

            let repo = Repository::clone(repo_info.clone(), Some(&reporter))
                .inspect_err(|e| {
                    progress_bar.abandon_with_message(format!("Failed to clone repository: {}", e))
                })
                .inspect(|_| {
                    progress_bar.finish_with_message(format!(
                        "Repository cloned: {}/{}",
                        repo_info.owner, repo_info.name
                    ))
                })?;

            Ok((repo.path().clone(), Some(path.to_string()), Some(repo_info)))
        }

        // Pull if exists
        (GitCachePolicy::AlwaysPull, true) => {
            progress_bar.set_prefix("ðŸ”„ Updating");
            progress_bar.set_message(format!(
                "Updating repository: {}/{}",
                repo_info.owner, repo_info.name
            ));

            let reporter = ProgressBarAdapter {
                bar: progress_bar,
                repo_info: &repo_info,
                is_clone: false,
            };

            let mut repo = Repository::open(repo_info.clone()).inspect_err(|e| {
                progress_bar.abandon_with_message(format!("Failed to open repository: {}", e));
            })?;

            repo.pull(Some(&reporter)).inspect_err(|e| {
                progress_bar.abandon_with_message(format!("Failed to update repository: {}", e))
            })?;

            progress_bar.finish_with_message(format!(
                "Repository updated: {}/{}",
                repo_info.owner, repo_info.name
            ));

            Ok((repo.path().clone(), Some(path.to_string()), Some(repo_info)))
        }

        // Use cache without pulling
        (GitCachePolicy::UseCache, true) => {
            progress_bar.set_prefix("ðŸ“‚ Using cached");
            progress_bar.set_message(format!(
                "Using cached repository: {}/{}",
                repo_info.owner, repo_info.name
            ));

            progress_bar.finish_with_message(format!(
                "Using cached repository: {}/{}",
                repo_info.owner, repo_info.name
            ));

            Ok((
                repo_info.cache_path.clone(),
                Some(path.to_string()),
                Some(repo_info),
            ))
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::string::ToString;

    #[test]
    fn test_git_host_display() {
        assert_eq!(GitHost::GitHub.to_string(), "GitHub");
        assert_eq!(GitHost::GitLab.to_string(), "GitLab");
        assert_eq!(GitHost::Bitbucket.to_string(), "Bitbucket");
        assert_eq!(
            GitHost::Other("custom.com".to_string()).to_string(),
            "custom.com"
        );
    }

    #[test]
    fn test_git_repo_info_display() {
        let info = GitRepoInfo {
            url: "https://github.com/username/repo".to_string(),
            host: GitHost::GitHub,
            owner: "username".to_string(),
            name: "repo".to_string(),
            cache_path: PathBuf::from("/tmp/cache/github/username/repo"),
        };

        assert_eq!(info.to_string(), "GitHub/username/repo");
    }
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/provider/anthropic.rs
================================================

```rs
//! Anthropic Claude tokenizer implementation

use reqwest::blocking::Client;
use serde::Deserialize;
use serde_json::json;
use std::env;

use super::Provider;
use crate::tokenizer::error::{TokenizerError, TokenizerResult};
use crate::tokenizer::model::Model;

/// Claude tokenizer implementation
pub struct ClaudeProvider {
    model: Model,
    client: Client,
}

impl ClaudeProvider {
    /// Create a new Claude tokenizer
    pub fn new(model: Model) -> Self {
        Self {
            model,
            client: Client::new(),
        }
    }
}

impl Provider for ClaudeProvider {
    fn count_tokens(&self, text: &str) -> TokenizerResult<usize> {
        // Get API key from environment
        let api_key = env::var("ANTHROPIC_API_KEY").map_err(|_| {
            TokenizerError::EnvVarError(
                "ANTHROPIC_API_KEY environment variable not set".to_string(),
            )
        })?;

        // Send request to token counting endpoint
        let response = self
            .client
            .post("https://api.anthropic.com/v1/messages/count_tokens")
            .header("x-api-key", api_key)
            .header("content-type", "application/json")
            .header("anthropic-version", "2023-06-01")
            .json(&json!({
                "model": self.model.model_id(),
                "messages": [{
                    "role": "user",
                    "content": text
                }]
            }))
            .send()?;

        // Check response status
        if !response.status().is_success() {
            let status = response.status();
            let error_text = response
                .text()
                .unwrap_or_else(|_| "Unable to read error message".to_string());

            return Err(TokenizerError::ApiError(format!(
                "Claude API returned error status {}: {}",
                status, error_text
            )));
        }

        // Parse the response
        #[derive(Deserialize)]
        struct TokenResponse {
            input_tokens: usize,
        }

        let token_response: TokenResponse = response.json()?;

        Ok(token_response.input_tokens)
    }

    fn model_context_window(&self) -> usize {
        self.model.context_window()
    }
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/provider/huggingface.rs
================================================

```rs
//! HuggingFace tokenizer implementation

use once_cell::sync::OnceCell;
use std::sync::Mutex;
use tokenizers::Tokenizer as HfTokenizer;

use super::Provider;
use crate::tokenizer::error::{TokenizerError, TokenizerResult};
use crate::tokenizer::model::Model;

/// HuggingFace tokenizer implementation
pub struct HuggingFaceProvider {
    model: Model,
    repo_id: &'static str,
    tokenizer: OnceCell<Mutex<HfTokenizer>>,
}

impl HuggingFaceProvider {
    /// Create a new HuggingFace tokenizer
    pub fn new(model: Model) -> Self {
        Self {
            model,
            repo_id: model.model_id(),
            tokenizer: OnceCell::new(),
        }
    }

    /// Get or initialize the tokenizer
    fn get_tokenizer(&self) -> TokenizerResult<&Mutex<HfTokenizer>> {
        self.tokenizer.get_or_try_init(|| {
            // Try to load the tokenizer from HuggingFace
            let tokenizer = match HfTokenizer::from_pretrained(self.repo_id, None) {
                Ok(t) => t,
                Err(e) => {
                    // Fall back to a basic BPE tokenizer
                    eprintln!("Error loading tokenizer: {}, using fallback", e);
                    let mut tokenizer = HfTokenizer::new(tokenizers::models::bpe::BPE::default());

                    // Configure for LLaMA-like tokenization
                    tokenizer.with_pre_tokenizer(Some(
                        tokenizers::pre_tokenizers::whitespace::Whitespace,
                    ));

                    tokenizer
                }
            };

            Ok(Mutex::new(tokenizer))
        })
    }
}

impl Provider for HuggingFaceProvider {
    fn count_tokens(&self, text: &str) -> TokenizerResult<usize> {
        // Get the tokenizer
        let tokenizer_mutex = self.get_tokenizer()?;

        // Acquire the lock
        let tokenizer = tokenizer_mutex
            .lock()
            .map_err(|_| TokenizerError::TokenizerError("Failed to lock tokenizer".to_string()))?;

        // Encode the text
        let encoding = tokenizer
            .encode(text, false)
            .map_err(|e| TokenizerError::TokenizerError(format!("Failed to encode text: {}", e)))?;

        // Get the token count
        let tokens = encoding.get_ids().len();

        Ok(tokens)
    }

    fn model_context_window(&self) -> usize {
        self.model.context_window()
    }
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/provider/mod.rs
================================================

```rs
//! Provider implementations for different tokenizer backends

pub mod anthropic;
pub mod huggingface;
pub mod openai;

use crate::tokenizer::error::TokenizerResult;

/// Trait for tokenizer provider implementations
pub trait Provider: Send + Sync {
    /// Count tokens in the given text
    fn count_tokens(&self, text: &str) -> TokenizerResult<usize>;

    /// Get the context window size for this model
    fn model_context_window(&self) -> usize;
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/provider/openai.rs
================================================

```rs
//! OpenAI tokenizer implementation using tiktoken

use tiktoken_rs::CoreBPE;

use super::Provider;
use crate::tokenizer::error::{TokenizerError, TokenizerResult};
use crate::tokenizer::model::Model;

/// OpenAI tokenizer implementation
pub struct OpenAIProvider {
    model: Model,
    encoding: CoreBPE,
}

impl OpenAIProvider {
    /// Create a new OpenAI tokenizer
    pub fn new(model: Model) -> TokenizerResult<Self> {
        let encoding = tiktoken_rs::get_bpe_from_model(model.model_id())
            .map_err(|e| TokenizerError::TokenizerError(e.to_string()))?;

        Ok(Self { model, encoding })
    }
}

impl Provider for OpenAIProvider {
    fn count_tokens(&self, text: &str) -> TokenizerResult<usize> {
        let tokens = self.encoding.encode_ordinary(text);
        Ok(tokens.len())
    }

    fn model_context_window(&self) -> usize {
        self.model.context_window()
    }
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/cache.rs
================================================

```rs
//! Token cache implementation

use std::collections::hash_map::DefaultHasher;
use std::fs;
use std::hash::{Hash, Hasher};
use std::path::PathBuf;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::{SystemTime, UNIX_EPOCH};

use serde::{Deserialize, Serialize};

use crate::tokenizer::error::{TokenizerError, TokenizerResult};

// Global cache statistics for easier access
static CACHE_HITS: AtomicUsize = AtomicUsize::new(0);
static CACHE_MISSES: AtomicUsize = AtomicUsize::new(0);

/// Cache entry with token count and model identifier
#[derive(Debug, Clone, Serialize, Deserialize)]
struct TokenCacheEntry {
    /// Hash of the content
    hash: u64,
    /// Model used for tokenization
    model: String,
    /// Token count
    tokens: usize,
    /// Timestamp when the entry was created
    timestamp: u64,
}

/// Statistics for token cache
#[derive(Debug, Clone, Copy, Default)]
pub struct CacheStats {
    /// Number of cache hits
    pub hits: usize,
    /// Number of cache misses
    pub misses: usize,
}

impl CacheStats {
    /// Get global cache statistics
    pub fn global() -> Self {
        Self {
            hits: CACHE_HITS.load(Ordering::Relaxed),
            misses: CACHE_MISSES.load(Ordering::Relaxed),
        }
    }
}

/// Cache for token counts to avoid redundant processing
#[derive(Debug, Serialize, Deserialize)]
pub struct TokenCache {
    /// Cached token entries
    entries: Vec<TokenCacheEntry>,

    #[serde(skip)]
    local_hits: usize,

    #[serde(skip)]
    local_misses: usize,
}

impl TokenCache {
    /// Create a new token cache
    pub fn new(project_dir: &str) -> TokenizerResult<Self> {
        // Try to load from disk, fall back to empty cache
        let cache = Self::load(project_dir).unwrap_or_else(|_| Self {
            entries: Vec::new(),
            local_hits: 0,
            local_misses: 0,
        });

        // Clean old entries
        cache.clean_old_entries(project_dir).ok();

        Ok(cache)
    }

    /// Calculate hash for content
    fn hash_content(&self, content: &str) -> u64 {
        let mut hasher = DefaultHasher::new();
        content.hash(&mut hasher);
        hasher.finish()
    }

    /// Get token count from cache if available
    pub fn get(&mut self, content: &str, model_id: &str) -> Option<usize> {
        let hash = self.hash_content(content);

        // Find matching entry
        let result = self
            .entries
            .iter()
            .find(|entry| entry.hash == hash && entry.model == model_id)
            .map(|entry| entry.tokens);

        // Update statistics
        if result.is_some() {
            self.local_hits += 1;
            CACHE_HITS.fetch_add(1, Ordering::Relaxed);
        } else {
            self.local_misses += 1;
            CACHE_MISSES.fetch_add(1, Ordering::Relaxed);
        }

        result
    }

    /// Insert token count into cache
    pub fn insert(
        &mut self,
        content: &str,
        model_id: &str,
        count: usize,
        project_dir: &str,
    ) -> TokenizerResult<()> {
        let hash = self.hash_content(content);

        // Remove existing entry with same hash and model
        self.entries
            .retain(|entry| !(entry.hash == hash && entry.model == model_id));

        // Add new entry
        self.entries.push(TokenCacheEntry {
            hash,
            model: model_id.to_string(),
            tokens: count,
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
        });

        // Save cache to disk
        self.save(project_dir)?;

        Ok(())
    }

    /// Get cache statistics
    pub fn get_stats(&self) -> CacheStats {
        CacheStats {
            hits: self.local_hits,
            misses: self.local_misses,
        }
    }

    /// Clean old cache entries (older than 7 days)
    fn clean_old_entries(&self, project_dir: &str) -> TokenizerResult<()> {
        let mut cache = self.clone();

        // Current timestamp
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs();

        // 7 days in seconds
        const WEEK_IN_SECS: u64 = 7 * 24 * 60 * 60;

        // Remove entries older than a week
        let old_len = cache.entries.len();
        cache
            .entries
            .retain(|entry| now - entry.timestamp < WEEK_IN_SECS);

        // If we removed any entries, save the file
        if cache.entries.len() < old_len {
            cache.save(project_dir)?;
        }

        Ok(())
    }

    /// Load cache from disk
    pub fn load(project_dir: &str) -> TokenizerResult<Self> {
        let path = get_cache_path(project_dir)?;

        if !path.exists() {
            return Err(TokenizerError::CacheError(
                "Cache file not found".to_string(),
            ));
        }

        let content = fs::read_to_string(&path)?;
        let mut cache: Self = serde_json::from_str(&content)?;

        // Initialize counters
        cache.local_hits = 0;
        cache.local_misses = 0;

        Ok(cache)
    }

    /// Save cache to disk
    pub fn save(&self, project_dir: &str) -> TokenizerResult<()> {
        let content = serde_json::to_string(self)?;
        let path = get_cache_path(project_dir)?;

        // Create parent directory if it doesn't exist
        if let Some(parent) = path.parent() {
            fs::create_dir_all(parent)?;
        }

        fs::write(&path, content)?;

        Ok(())
    }

    /// Clone the cache for internal use
    fn clone(&self) -> Self {
        Self {
            entries: self.entries.clone(),
            local_hits: self.local_hits,
            local_misses: self.local_misses,
        }
    }
}

/// Get the path to the token cache file for a specific project directory
pub fn get_cache_path(project_dir: &str) -> TokenizerResult<PathBuf> {
    // Get home directory
    let home_dir = dirs::home_dir().ok_or_else(|| {
        TokenizerError::CacheError("Could not determine home directory".to_string())
    })?;

    // Create cache directory path
    let cache_dir = home_dir.join(".cache").join("dumpfs");

    // Create a sanitized filename based on the project directory path
    let canonical_path = fs::canonicalize(project_dir)
        .map_err(|e| TokenizerError::CacheError(format!("Invalid project directory: {}", e)))?;

    // Convert the path to a string, removing any invalid characters
    let path_str = canonical_path.to_string_lossy().to_string();
    let sanitized_path = path_str.replace(
        |c: char| !c.is_alphanumeric() && c != '_' && c != '-' && c != '.',
        "_",
    );

    // Create the cache file path
    let cache_file = cache_dir.join(format!("{}.token_cache.json", sanitized_path));

    Ok(cache_file)
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/error.rs
================================================

```rs
//! Error types for the tokenizer module

use std::io;
use thiserror::Error;

/// Result type for tokenizer operations
pub type TokenizerResult<T> = Result<T, TokenizerError>;

/// Errors that can occur during tokenization
#[derive(Error, Debug)]
pub enum TokenizerError {
    /// Error from API call
    #[error("API error: {0}")]
    ApiError(String),

    /// Error from tokenizer library
    #[error("Tokenizer error: {0}")]
    TokenizerError(String),

    /// Model is not supported
    #[error("Unsupported model: {0}")]
    UnsupportedModel(String),

    /// Required environment variable not set
    #[error("Environment variable not set: {0}")]
    EnvVarError(String),

    /// Cache operation error
    #[error("Cache error: {0}")]
    CacheError(String),

    /// Failed to acquire lock on cache
    #[error("Failed to acquire lock on cache")]
    CacheLockError,

    /// IO error
    #[error("IO error: {0}")]
    IoError(#[from] io::Error),

    /// JSON serialization/deserialization error
    #[error("JSON error: {0}")]
    JsonError(#[from] serde_json::Error),

    /// Request error
    #[error("Request error: {0}")]
    RequestError(String),
}

impl From<reqwest::Error> for TokenizerError {
    fn from(error: reqwest::Error) -> Self {
        TokenizerError::RequestError(error.to_string())
    }
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/mod.rs
================================================

```rs
//! Tokenizer module for token counting with different LLM models
//!
//! Handles tokenization for various LLM models from different providers
//! with efficient caching to improve performance.

mod cache;
mod error;
mod model;
mod provider;

// Re-exports for public API
pub use cache::CacheStats;
pub use error::{TokenizerError, TokenizerResult};
pub use model::{Model, ModelProvider};

use cache::TokenCache;
use provider::Provider;
use std::sync::{Arc, Mutex};

/// Result of token counting operation
#[derive(Debug, Clone, Copy)]
pub struct TokenCount {
    /// Number of tokens in the text
    pub tokens: usize,
    /// Whether this was a cache hit (if caching is enabled)
    pub cached: Option<bool>,
}

/// Trait defining the interface for tokenizers
pub trait Tokenizer: Send + Sync {
    /// Count tokens in the given text
    fn count_tokens(&self, text: &str) -> TokenizerResult<TokenCount>;

    /// Get the context window size for this model
    fn model_context_window(&self) -> usize;
}

/// Create a tokenizer for the specified model
pub fn create_tokenizer(model: Model, project_dir: &str) -> TokenizerResult<Box<dyn Tokenizer>> {
    // Create the appropriate provider based on model
    let provider: Box<dyn Provider> = match model.provider() {
        ModelProvider::Anthropic => Box::new(provider::anthropic::ClaudeProvider::new(model)),
        ModelProvider::OpenAI => Box::new(provider::openai::OpenAIProvider::new(model)?),
        ModelProvider::HuggingFace => {
            Box::new(provider::huggingface::HuggingFaceProvider::new(model))
        }
    };

    // Wrap with caching tokenizer
    let cache = Arc::new(Mutex::new(TokenCache::new(project_dir)?));

    Ok(Box::new(CachingTokenizer::new(
        provider,
        model,
        cache,
        project_dir.to_string(),
    )))
}

/// Get global cache statistics
pub fn get_global_cache_stats() -> CacheStats {
    CacheStats::global()
}

/// Tokenizer that caches results to avoid repeated tokenization
pub struct CachingTokenizer {
    provider: Box<dyn Provider>,
    model: Model,
    cache: Arc<Mutex<TokenCache>>,
    project_dir: String,
}

impl CachingTokenizer {
    /// Create a new cached tokenizer
    pub fn new(
        provider: Box<dyn Provider>,
        model: Model,
        cache: Arc<Mutex<TokenCache>>,
        project_dir: String,
    ) -> Self {
        Self {
            provider,
            model,
            cache,
            project_dir,
        }
    }

    /// Get cache statistics
    pub fn get_cache_stats(&self) -> CacheStats {
        if let Ok(cache) = self.cache.lock() {
            cache.get_stats()
        } else {
            CacheStats::default()
        }
    }
}

impl Tokenizer for CachingTokenizer {
    fn count_tokens(&self, text: &str) -> TokenizerResult<TokenCount> {
        let model_id = self.model.model_id();

        // Try to get from cache
        let cached = self
            .cache
            .lock()
            .map_err(|_| TokenizerError::CacheLockError)?
            .get(text, model_id);

        // If found in cache, return it
        if let Some(count) = cached {
            return Ok(TokenCount {
                tokens: count,
                cached: Some(true),
            });
        }

        // If not in cache, use the provider
        let result = self.provider.count_tokens(text)?;

        // Update cache
        self.cache
            .lock()
            .map_err(|_| TokenizerError::CacheLockError)?
            .insert(text, model_id, result, &self.project_dir)?;

        Ok(TokenCount {
            tokens: result,
            cached: Some(false),
        })
    }

    fn model_context_window(&self) -> usize {
        self.model.context_window()
    }
}

#[cfg(test)]
mod tests {
    use tempfile::tempdir;

    use super::*;
    use crate::tokenizer::error::TokenizerResult;
    use std::env;

    // No unused mock types needed

    // Simple mock tokenizer that doesn't rely on external dependencies
    struct MockTokenizer {
        context_window: usize,
    }

    impl Tokenizer for MockTokenizer {
        fn count_tokens(&self, _text: &str) -> TokenizerResult<TokenCount> {
            Ok(TokenCount {
                tokens: 42,
                cached: None,
            })
        }

        fn model_context_window(&self) -> usize {
            self.context_window
        }
    }

    #[test]
    fn test_create_tokenizer() {
        // Use a mock tokenizer directly to avoid external dependencies
        let tokenizer = MockTokenizer {
            context_window: 8192,
        };

        // Test the context window
        assert_eq!(tokenizer.model_context_window(), 8192);

        // Test token counting
        let result = tokenizer.count_tokens("Hello, world!");
        assert!(result.is_ok());

        let count = result.expect("Token count should be valid in test environment");
        assert_eq!(count.tokens, 42);
    }

    #[test]
    fn test_tokenizer_caching() {
        // Create an in-memory cache that doesn't touch the filesystem
        struct InMemoryCache {
            storage: std::collections::HashMap<String, usize>,
        }

        impl InMemoryCache {
            fn new() -> Self {
                Self {
                    storage: std::collections::HashMap::new(),
                }
            }

            fn get(&mut self, key: &str) -> Option<usize> {
                self.storage.get(key).copied()
            }

            fn insert(&mut self, key: &str, value: usize) {
                self.storage.insert(key.to_string(), value);
            }
        }

        // Create a simple tokenizer with caching logic for testing
        struct TestTokenizer {
            cache: InMemoryCache,
        }

        impl TestTokenizer {
            fn new() -> Self {
                Self {
                    cache: InMemoryCache::new(),
                }
            }

            fn count_tokens(&mut self, text: &str) -> (usize, bool) {
                // Check cache first
                if let Some(count) = self.cache.get(text) {
                    return (count, true); // Cache hit
                }

                // "Calculate" tokens
                let count = 42;

                // Store in cache
                self.cache.insert(text, count);

                (count, false) // Cache miss
            }
        }

        // Test the caching logic
        let mut tokenizer = TestTokenizer::new();

        // First call should be a cache miss
        let (count1, cached1) = tokenizer.count_tokens("Hello, world!");
        assert_eq!(count1, 42);
        assert!(!cached1);

        // Second call should be a cache hit
        let (count2, cached2) = tokenizer.count_tokens("Hello, world!");
        assert_eq!(count2, 42);
        assert!(cached2);
    }

    #[test]
    #[ignore] // Skip this test by default since it requires an API key
    fn test_claude_tokenizer() {
        let tmp_dir = tempdir().expect("Should create temp dir");
        let test_dir = tmp_dir.path().to_str().unwrap();

        // Only run this test if ANTHROPIC_API_KEY is set
        match env::var("ANTHROPIC_API_KEY") {
            Ok(api_key) if !api_key.is_empty() => {
                let tokenizer = create_tokenizer(Model::Sonnet37, test_dir)
                    .expect("Tokenizer creation should succeed when API key is set");
                let result = tokenizer.count_tokens("Hello, Claude!");
                let count = result.expect("Token count should be valid with valid API key");
                assert!(count.tokens > 0);
            }
            _ => {
                // Skip test when API key is not available
                println!("Skipping Claude tokenizer test (no API key)");
            }
        }
    }
}

```

================================================
/home/user/projs/dumpfs/src/tokenizer/model.rs
================================================

```rs
//! Model definitions and metadata

use clap::ValueEnum;
use serde::{Deserialize, Serialize};
use std::str::FromStr;
use strum::{Display, EnumIter, EnumProperty, EnumString};

/// Supported LLM models for tokenization
#[derive(
    Debug,
    Clone,
    Copy,
    PartialEq,
    Eq,
    EnumIter,
    Display,
    ValueEnum,
    Serialize,
    Deserialize,
    EnumProperty,
)]
pub enum Model {
    #[strum(props(
        model_id = "claude-3-5-sonnet-latest",
        context_window = "200000",
        provider = "anthropic"
    ))]
    Sonnet35,

    #[strum(props(
        model_id = "claude-3-7-sonnet-latest",
        context_window = "200000",
        provider = "anthropic"
    ))]
    Sonnet37,

    // OpenAI models
    #[strum(props(model_id = "gpt-4", context_window = "8192", provider = "openai"))]
    Gpt4,

    #[strum(props(
        model_id = "gpt-4-0125-preview",
        context_window = "128000",
        provider = "openai"
    ))]
    Gpt4Turbo,

    #[strum(props(model_id = "gpt-4o", context_window = "8192", provider = "openai"))]
    Gpt4o,

    // HuggingFace models
    #[strum(props(
        model_id = "meta-llama/Llama-2-7b-hf",
        context_window = "4096",
        provider = "huggingface"
    ))]
    Llama2_7b,

    #[strum(props(
        model_id = "meta-llama/Llama-3-8b-hf",
        context_window = "8192",
        provider = "huggingface"
    ))]
    Llama3_8b,

    #[strum(props(
        model_id = "mistralai/Mistral-Small-3.1-24B-Base-2503",
        context_window = "128000",
        provider = "huggingface"
    ))]
    MistralSmall24B,

    #[strum(props(
        model_id = "mistralai/Mistral-Large-Instruct-2411",
        context_window = "128000",
        provider = "huggingface"
    ))]
    MistralLargeInstruct,

    #[strum(props(
        model_id = "mistralai/Pixtral-12B-Base-2409",
        context_window = "128000",
        provider = "huggingface"
    ))]
    Pixtral12B,

    #[strum(props(
        model_id = "mistralai/Mistral-Small-Instruct-2409",
        context_window = "32000",
        provider = "huggingface"
    ))]
    MistralSmall,
}

impl Model {
    /// Get the context window size for this model
    pub fn context_window(&self) -> usize {
        self.get_int("context_window").unwrap_or(0) as usize
    }

    /// Get the provider of this model
    pub fn provider(&self) -> ModelProvider {
        let provider = self.get_str("provider").unwrap_or("unknown");
        ModelProvider::from_str(provider).unwrap_or(ModelProvider::HuggingFace)
    }

    /// Get the model identifier as used by the provider's API
    pub fn model_id(&self) -> &'static str {
        self.get_str("model_id").unwrap_or("unknown")
    }
}

/// Model providers
#[derive(Debug, Clone, Copy, PartialEq, Eq, EnumString, Display)]
#[strum(serialize_all = "lowercase")]
pub enum ModelProvider {
    /// Anthropic (Claude models)
    Anthropic,
    /// OpenAI (GPT models)
    OpenAI,
    /// HuggingFace models
    HuggingFace,
}

```

================================================
/home/user/projs/dumpfs/src/types.rs
================================================

```rs
/*!
 * Core types and data structures for the DumpFS application
 */

use std::path::PathBuf;
use std::time::SystemTime;

/// Represents different types of filesystem entries
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum FileType {
    /// Text file with readable content
    TextFile,
    /// Binary file (non-text)
    BinaryFile,
    /// Symbolic link to another file
    Symlink,
    /// Directory containing other entries
    Directory,
    /// Other file types
    Other,
}

/// Metadata about a filesystem entry
#[derive(Debug, Clone)]
pub struct Metadata {
    /// Size in bytes
    pub size: u64,
    /// Last modification time
    pub modified: SystemTime,
    /// File permissions in octal format
    pub permissions: String,
}

/// Represents a directory in the file system
#[derive(Debug, Clone)]
pub struct DirectoryNode {
    /// Directory name
    pub name: String,
    /// Relative path from scan root
    pub path: PathBuf,
    /// Directory metadata
    pub metadata: Metadata,
    /// Directory contents
    pub contents: Vec<Node>,
}

/// Represents a text file
#[derive(Debug, Clone)]
pub struct FileNode {
    /// File name
    pub name: String,
    /// Relative path from scan root
    pub path: PathBuf,
    /// File metadata
    pub metadata: Metadata,
    /// File content (may be None if too large)
    pub content: Option<String>,
}

/// Represents a binary file
#[derive(Debug, Clone)]
pub struct BinaryNode {
    /// File name
    pub name: String,
    /// Relative path from scan root
    pub path: PathBuf,
    /// File metadata
    pub metadata: Metadata,
}

/// Represents a symbolic link
#[derive(Debug, Clone)]
pub struct SymlinkNode {
    /// Link name
    pub name: String,
    /// Relative path from scan root
    pub path: PathBuf,
    /// Link metadata
    pub metadata: Metadata,
    /// Target of the symlink
    pub target: String,
}

/// A generic filesystem node
#[derive(Debug, Clone)]
pub enum Node {
    /// Directory node
    Directory(DirectoryNode),
    /// Text file node
    File(FileNode),
    /// Binary file node
    Binary(BinaryNode),
    /// Symbolic link node
    Symlink(SymlinkNode),
}

```

================================================
/home/user/projs/dumpfs/src/utils.rs
================================================

```rs
/*!
 * Utility functions for DumpFS
 */

use std::io;
use std::path::Path;
use std::sync::Arc;

use ignore::WalkBuilder;
use indicatif::ProgressBar;
use once_cell::sync::Lazy;
use walkdir::WalkDir;

use crate::config::Config;
use crate::scanner::Scanner;

/// Count total files for progress tracking
pub fn count_files(dir: &Path, config: &Config) -> io::Result<u64> {
    let scanner = Scanner::new(config.clone(), Arc::new(ProgressBar::hidden()));
    let mut count = 0;

    if config.respect_gitignore {
        // Use ignore crate's Walk to handle .gitignore patterns
        let mut walker = WalkBuilder::new(dir);

        // Custom gitignore file if specified
        if let Some(gitignore_path) = &config.gitignore_path {
            walker.add_custom_ignore_filename(gitignore_path);
        }

        for entry in walker.build().filter_map(Result::ok) {
            if entry.file_type().is_some_and(|ft| ft.is_file())
                && !scanner.should_ignore(entry.path())
                && scanner.should_include(entry.path())
            {
                count += 1;
            }
        }
    } else {
        // Use walkdir without gitignore support
        for entry in WalkDir::new(dir).into_iter().filter_map(Result::ok) {
            if entry.file_type().is_file()
                && !scanner.should_ignore(entry.path())
                && scanner.should_include(entry.path())
            {
                count += 1;
            }
        }
    }

    Ok(count)
}

/// Format a human-readable file size
pub fn format_file_size(size: u64) -> String {
    const KB: u64 = 1024;
    const MB: u64 = KB * 1024;
    const GB: u64 = MB * 1024;

    if size >= GB {
        format!("{:.2} GB", size as f64 / GB as f64)
    } else if size >= MB {
        format!("{:.2} MB", size as f64 / MB as f64)
    } else if size >= KB {
        format!("{:.2} KB", size as f64 / KB as f64)
    } else {
        format!("{} bytes", size)
    }
}

/// Default patterns to ignore
pub static DEFAULT_IGNORE: Lazy<Vec<&'static str>> = Lazy::new(|| {
    vec![
        // Version Control
        ".git",
        ".svn",
        ".hg",
        ".bzr",
        ".gitignore",
        ".gitattributes",
        // OS Files
        ".DS_Store",
        "Thumbs.db",
        "desktop.ini",
        "ehthumbs.db",
        "*.lnk",
        "*.url",
        ".directory",
        // Dependencies
        "node_modules",
        "bower_components",
        ".npm",
        "package-lock.json",
        "yarn.lock",
        "pnpm-lock.yaml",
        ".pnpm",
        ".yarn",
        "yarn-error.log",
        "vendor",
        "composer.lock",
        ".pnpm-store",
        "npm-shrinkwrap.json",
        "lerna-debug.log",
        ".yalc",
        ".turbo",
        // Build & Dist
        "dist",
        "build",
        "out",
        "bin",
        "release",
        "*.min.js",
        "*.min.css",
        "bundle.*",
        ".parcel-cache",
        ".next",
        ".nuxt",
        ".output",
        ".cache",
        ".rollup.cache",
        ".webpack",
        ".serverless",
        ".netlify",
        "storybook-static",
        // Python
        "__pycache__",
        ".pytest_cache",
        ".coverage",
        "venv",
        "env",
        ".env",
        ".venv",
        "*.pyc",
        "*.pyo",
        "*.pyd",
        ".python-version",
        "*.egg-info",
        "*.egg",
        "develop-eggs",
        // Rust
        "target",
        "Cargo.lock",
        ".cargo",
        // IDEs & Editors
        ".idea",
        ".vscode",
        ".vs",
        ".sublime-*",
        "*.swp",
        "*.swo",
        "*~",
        ".project",
        ".settings",
        ".classpath",
        ".factorypath",
        "*.iml",
        "*.iws",
        "*.ipr",
        // Caches & Temp
        ".cache",
        "tmp",
        "temp",
        "logs",
        ".sass-cache",
        ".eslintcache",
        "*.log",
        "npm-debug.log*",
        "yarn-debug.log*",
        "yarn-error.log*",
        // Other Build Tools
        ".gradle",
        "gradle",
        ".maven",
        ".m2",
        "*.class",
        "*.jar",
        "*.war",
        "*.ear",
        // JavaScript/TypeScript
        "coverage",
        ".nyc_output",
        ".next",
        "*.tsbuildinfo",
        ".nuxt",
        ".output",
        // .NET
        "bin",
        "obj",
        "Debug",
        "Release",
        "packages",
        "*.suo",
        "*.user",
        "*.pubxml",
        "*.pubxml.user",
        // Documentation
        "_site",
        ".jekyll-cache",
        ".docusaurus",
        // Mobile Development
        ".gradle",
        "build",
        "xcuserdata",
        "*.xcworkspace",
        "Pods/",
        ".expo",
        // Database
        "*.sqlite",
        "*.sqlite3",
        "*.db",
        // Archives
        "*.zip",
        "*.tar.gz",
        "*.tgz",
        "*.rar",
        // Kubernetes
        ".kube",
        "*.kubeconfig",
        // Terraform
        ".terraform",
        "*.tfstate",
        "*.tfvars",
        // Ansible
        "*.retry",
    ]
});

```

================================================
/home/user/projs/dumpfs/src/config.rs
================================================

```rs
/*!
 * Configuration handling for DumpFS
 */

use std::io;
use std::path::PathBuf;

use clap::Parser;
use clap_complete::Shell;

use crate::git::{GitCachePolicy, GitRepoInfo};
use crate::tokenizer::Model;
use crate::FsWriterFormatter;

/// Command-line arguments for DumpFS
#[derive(Parser, Debug, Clone)]
#[clap(
    name = "dumpfs",
    version = env!("CARGO_PKG_VERSION"),
    about = "Generate XML representation of directory contents for LLM context",
    long_about = "Creates an XML representation of a directory structure and its contents, designed for providing context to Large Language Models (LLMs)."
)]
pub struct Args {
    /// Target directory or Git repository URL to process
    #[clap(default_value = ".")]
    pub directory_path: String,

    /// Output XML file name
    pub output_file: Option<String>,

    /// Comma-separated list of patterns to ignore
    #[clap(long, value_delimiter = ',')]
    pub ignore_patterns: Vec<String>,

    /// Comma-separated list of patterns to include (if specified, only matching files are included)
    #[clap(long, value_delimiter = ',')]
    pub include_patterns: Vec<String>,

    /// Number of threads to use for processing
    #[clap(long, default_value = "4")]
    pub threads: usize,

    /// Respect .gitignore files (default: true)
    #[clap(long, default_value = "true")]
    pub respect_gitignore: bool,

    /// Path to custom .gitignore file
    #[clap(long)]
    pub gitignore_path: Option<String>,

    /// Include file and directory metadata (size, modified time, permissions)
    #[clap(long, help = "Include file and directory metadata in the XML output")]
    pub include_metadata: bool,

    /// LLM model to use for tokenization (enables token counting)
    #[clap(long, value_enum)]
    pub model: Option<Model>,

    /// Generate shell completions
    #[clap(long = "generate", value_enum)]
    pub generate: Option<Shell>,

    /// Clean Git repository cache (specify number of days, 0 for all)
    #[clap(long, value_name = "DAYS")]
    pub clean_cache: Option<u64>,

    /// Policy for handling Git repository caching
    #[clap(long, value_enum, default_value_t = GitCachePolicy::default())]
    pub git_cache_policy: GitCachePolicy,

    /// Copy output to clipboard
    #[clap(long, help = "Copy output to system clipboard")]
    pub clip: bool,
    /// Copy output to clipboard
    #[clap(long, help = "print to stdout")]
    pub stdout: bool,

    /// Writer format
    #[clap(long, short)]
    pub format: Option<FsWriterFormatter>,
}

/// Application configuration
#[derive(Clone, Debug)]
pub struct Config {
    /// Target directory to process
    pub target_dir: PathBuf,

    /// Output XML file path
    pub output_file: PathBuf,

    /// Patterns to ignore
    pub ignore_patterns: Vec<String>,

    /// Patterns to include (if empty, include all)
    pub include_patterns: Vec<String>,

    /// Number of threads to use for processing
    pub num_threads: usize,

    /// Whether to respect .gitignore files
    pub respect_gitignore: bool,

    /// Path to custom .gitignore file
    pub gitignore_path: Option<PathBuf>,

    /// LLM model to use for tokenization
    pub model: Option<Model>,

    /// Original repository URL (if applicable)
    pub repo_url: Option<String>,

    /// Git repository information (if applicable)
    pub git_repo: Option<GitRepoInfo>,

    /// Policy for handling Git repository caching
    pub git_cache_policy: GitCachePolicy,

    /// Include file and directory metadata
    pub include_metadata: bool,

    /// Copy output to clipboard
    pub clip: bool,

    /// Copy output to clipboard
    pub stdout: bool,

    pub format: FsWriterFormatter,
}

impl Config {
    /// Create configuration from command-line arguments
    pub fn from_args(args: Args) -> Self {
        let format = args.format.unwrap_or_default();
        let target_dir = PathBuf::from(args.directory_path.clone());
        let output_file = args.output_file.map(Into::into).unwrap_or(match format {
            FsWriterFormatter::Xml => target_dir.join(".dumpfs.context.xml"),
            FsWriterFormatter::Txt => target_dir.join(".dumpfs.context.md"),
        });
        Self {
            target_dir,
            output_file,
            ignore_patterns: args.ignore_patterns,
            include_patterns: args.include_patterns,
            num_threads: args.threads,
            respect_gitignore: args.respect_gitignore,
            gitignore_path: args.gitignore_path.map(PathBuf::from),
            model: args.model,
            repo_url: None,
            git_repo: None,
            git_cache_policy: args.git_cache_policy,
            include_metadata: args.include_metadata,
            stdout: args.stdout,
            clip: args.clip,
            format,
        }
    }

    /// Validate the configuration
    pub fn validate(&self) -> io::Result<()> {
        // For Git repositories, we've already validated during cloning
        if self.repo_url.is_some() && self.git_repo.is_some() {
            // Check if the cloned directory exists and is readable
            if !self.target_dir.exists() || !self.target_dir.is_dir() {
                return Err(io::Error::new(
                    io::ErrorKind::NotFound,
                    format!(
                        "Cloned repository directory not found: {}",
                        self.target_dir.display()
                    ),
                ));
            }
        } else {
            // For local directories, check if target directory exists and is readable
            if !self.target_dir.exists() || !self.target_dir.is_dir() {
                return Err(io::Error::new(
                    io::ErrorKind::NotFound,
                    format!("Target directory not found: {}", self.target_dir.display()),
                ));
            }
        }

        // Check if output file directory exists and is writable
        if let Some(parent) = self.output_file.parent() {
            if !parent.exists() && parent != PathBuf::from("") {
                return Err(io::Error::new(
                    io::ErrorKind::NotFound,
                    format!("Output directory not found: {}", parent.display()),
                ));
            }
        }

        // Check if custom gitignore file exists
        if let Some(path) = &self.gitignore_path {
            if !path.exists() {
                return Err(io::Error::new(
                    io::ErrorKind::NotFound,
                    format!("Custom .gitignore file not found: {}", path.display()),
                ));
            }
        }

        Ok(())
    }
}

```

================================================
/home/user/projs/dumpfs/src/lib.rs
================================================

```rs
/*!
 * DumpFS - Generate XML representation of directory contents for LLM context
 *
 * This library creates structured XML representations of directory contents
 * for use as context for Large Language Models.
 */

pub mod clipboard;
pub mod config;
pub mod error;
pub mod git;
pub mod report;
pub mod scanner;
pub mod tokenizer;
pub mod types;
pub mod utils;
pub mod writer;

#[cfg(test)]
mod tests;

// Re-export main components for easier access
pub use clipboard::{copy_to_clipboard, ClipboardError};
pub use config::Config;
pub use error::{DumpFsError, Result, ResultExt};
pub use report::{FileReportInfo, ReportFormat, Reporter, ScanReport};
pub use scanner::Scanner;
pub use types::{BinaryNode, DirectoryNode, FileNode, FileType, Metadata, Node, SymlinkNode};
pub use utils::{count_files, format_file_size};
pub use writer::FsWriterFormatter;

// No process_path export needed

/// Version of the library
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

```

================================================
/home/user/projs/dumpfs/src/main.rs
================================================

```rs
/*!
 * Command-line interface for DumpFS
 */

use std::io::{self, Write};
use std::path::Path;
use std::sync::Arc;
use std::time::Instant;

use dumpfs::clipboard;
use dumpfs::error::{DumpFsError, Result};

use clap::{CommandFactory, Parser};
use clap_complete::{generate, CompleteEnv, Shell};
use indicatif::{ProgressBar, ProgressStyle};
use rayon::ThreadPoolBuilder;

use dumpfs::config::{Args, Config};
use dumpfs::git;
use dumpfs::report::{ReportFormat, Reporter, ScanReport};
use dumpfs::scanner::Scanner;
use dumpfs::utils::count_files;

/// Generate shell completions
fn print_completions(generator: Shell, cmd: &mut clap::Command) {
    generate(
        generator,
        cmd,
        cmd.get_name().to_string(),
        &mut io::stdout(),
    );
}

fn main() -> Result<()> {
    // Enable automatic shell completion
    CompleteEnv::with_factory(Args::command).complete();

    // Parse command line arguments
    let args = Args::parse();

    // Handle completions if requested
    if let Some(generator) = args.generate {
        let mut cmd = Args::command();
        eprintln!("Generating completion file for {generator:?}...");
        print_completions(generator, &mut cmd);
        return Ok(());
    }

    // Handle cache cleaning if requested
    if let Some(days) = args.clean_cache {
        eprintln!(
            "Cleaning Git repository cache (older than {} days)...",
            days
        );
        match git::clean_cache(days) {
            Ok(count) => {
                eprintln!("Removed {} repositories from cache", count);
                return Ok(());
            }
            Err(e) => {
                eprintln!("Error cleaning cache: {}", e);
                return Err(DumpFsError::Io(e));
            }
        }
    }

    // Create progress bar with advanced Unicode styling
    let progress = ProgressBar::new(0);
    progress.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} {prefix:.bold.cyan} {wide_msg:.dim.white} {pos}/{len} ({percent}%) â±ï¸  Elapsed: {elapsed_precise}  Remaining: {eta_precise}  Speed: {per_sec}/s")
            .map_err(|e| DumpFsError::Unexpected(format!("Failed to create progress style: {}", e)))?
    );
    progress.enable_steady_tick(std::time::Duration::from_millis(100));
    progress.set_prefix("ðŸ“Š Setup");

    // Create initial configuration
    let mut config = Config::from_args(args.clone());

    // Process path (either local directory or git repository URL)
    progress.set_message(format!("Processing path: {}", args.directory_path));
    let (processed_path, repo_url, git_repo) = match git::process_path(
        &args.directory_path,
        config.git_cache_policy,
        Some(&progress),
    ) {
        Ok(result) => result,
        Err(e) => {
            progress.abandon_with_message(format!("Error processing path: {}", e));
            eprintln!("Error processing path: {}", e);
            return Err(e.into());
        }
    };

    // Update config with processed path and repo info
    config.target_dir = processed_path;
    config.repo_url = repo_url;
    config.git_repo = git_repo;

    // Adjust output file location for git repositories
    if let Some(repo) = &config.git_repo {
        // Check if output file is a relative path with no directory component
        if !config.output_file.is_absolute()
            && (config.output_file.parent().is_none()
                || config
                    .output_file
                    .parent()
                    .expect("Parent should be Some if not None")
                    == Path::new(""))
        {
            // Use the repository directory for the output file
            config.output_file = repo.cache_path.join(config.output_file);
        }
    }

    // Validate configuration
    config.validate()?;

    // Configure thread pool
    if let Err(e) = ThreadPoolBuilder::new()
        .num_threads(config.num_threads)
        .build_global()
    {
        eprintln!("Warning: Failed to set thread pool size: {}", e);
    }

    progress.set_message(format!(
        "ðŸ“‚ Scanning directory: {}",
        config.target_dir.display()
    ));

    // Add gitignore status message
    if config.respect_gitignore {
        progress.set_message(match &config.gitignore_path {
            Some(path) => format!("ðŸ” Using custom gitignore file: {}", path.display()),
            None => "ðŸ” Respecting .gitignore files in the project".to_string(),
        });
    }

    // Count files for progress tracking
    let total_files = match count_files(&config.target_dir, &config) {
        Ok(count) => {
            progress.set_message(format!("ðŸ”Ž Found {} files to process", count));
            count
        }
        Err(e) => {
            progress.set_message(format!("âš ï¸ Warning: Failed to count files: {}", e));
            0
        }
    };

    progress.set_length(total_files);
    progress.set_prefix("ðŸ“Š Processing");
    progress.set_message("Starting scan...");

    // Create scanner and writer
    let scanner = Scanner::new(config.clone(), Arc::new(progress.clone()));

    // Start timing both scan and write operations
    let start_time = Instant::now();

    // Scan directory
    let root_node = scanner.scan()?;

    // Write XML output
    config.format.write(config.clone(), &root_node)?;

    // Calculate total duration (scan + write)
    let total_duration = start_time.elapsed();

    // Clear the progress bar
    progress.finish_and_clear();

    // Get scanner statistics
    let scanner_stats = scanner.get_statistics()?;

    // Prepare the scan report
    let scan_report = ScanReport {
        output_file: config.output_file.display().to_string(),
        duration: total_duration,
        files_processed: scanner_stats.files_processed,
        total_lines: scanner_stats.total_lines,
        total_chars: scanner_stats.total_chars,
        total_tokens: scanner_stats.total_tokens,
        file_details: scanner_stats.file_details,
        token_cache_hits: scanner_stats.token_cache_hits,
        token_cache_misses: scanner_stats.token_cache_misses,
    };

    // Create a reporter and print the report
    let reporter = Reporter::new(ReportFormat::ConsoleTable);
    reporter.print_report(&scan_report);

    // Handle clipboard functionality if --clip is specified
    if config.clip || config.stdout {
        // Get the output file content
        let output_content = std::fs::read_to_string(&config.output_file)?;
        if config.stdout {
            std::io::stdout().write_all(output_content.as_bytes())?;
        }

        if config.clip {
            // Copy to clipboard
            match clipboard::copy_to_clipboard(&output_content) {
                Ok(_) => {
                    eprintln!("âœ… Output copied to clipboard successfully");
                }
                Err(e) => {
                    eprintln!("âŒ Failed to copy to clipboard: {}", e);
                    // Don't return error as the main functionality (file generation) succeeded
                }
            }
        }
    }

    Ok(())
}

```

================================================
/home/user/projs/dumpfs/src/report.rs
================================================

```rs
/*!
 * Reporting functionality for DumpFS
 *
 * Provides functionality for generating formatted reports of scan results
 * using the tabled library for clean, consistent table rendering.
 */

use std::collections::HashMap;
use std::time::Duration;

use tabled::{
    settings::{object::Columns, Alignment, Modify, Padding, Style},
    Table, Tabled,
};

/// Information about a file in the report
#[derive(Debug, Clone, Default)]
pub struct FileReportInfo {
    /// Number of lines in the file
    pub lines: usize,
    /// Number of characters in the file
    pub chars: usize,
    /// Number of tokens in the file (if tokenizer is enabled)
    pub tokens: Option<usize>,
}

/// Statistics for a directory scan
#[derive(Debug, Clone)]
pub struct ScanReport {
    /// Output file path
    pub output_file: String,
    /// Time taken to scan
    pub duration: Duration,
    /// Number of files processed
    pub files_processed: usize,
    /// Total number of lines
    pub total_lines: usize,
    /// Total number of characters
    pub total_chars: usize,
    /// Total number of tokens (if tokenizer is enabled)
    pub total_tokens: Option<usize>,
    /// Details for each file
    pub file_details: HashMap<String, FileReportInfo>,
    /// Token cache hits (if tokenizer caching is enabled)
    pub token_cache_hits: Option<usize>,
    /// Token cache misses (if tokenizer caching is enabled)
    pub token_cache_misses: Option<usize>,
}

/// Format of the report output
pub enum ReportFormat {
    /// Console table output
    ConsoleTable,
    // Other formats could be added in the future
    // JSON, HTML, etc.
}

/// Report generator for scan results
pub struct Reporter {
    format: ReportFormat,
}

impl Reporter {
    /// Create a new reporter
    pub fn new(format: ReportFormat) -> Self {
        Self { format }
    }

    /// Format a number with human-readable units
    fn format_number(&self, num: usize) -> String {
        if num >= 1_000_000 {
            format!("{:.1}M", num as f64 / 1_000_000.0)
        } else if num >= 1_000 {
            format!("{:.1}K", num as f64 / 1_000.0)
        } else {
            num.to_string()
        }
    }

    /// Generate a report string based on scan statistics
    pub fn generate_report(&self, report: &ScanReport) -> String {
        match self.format {
            ReportFormat::ConsoleTable => self.generate_console_report(report),
            // Additional formats could be added here
        }
    }

    /// Print the report to stdout
    pub fn print_report(&self, report: &ScanReport) {
        println!("\n{}", self.generate_report(report));
    }

    // Format path to be relative and handle truncation if needed
    fn format_path(&self, path: &str, max_len: usize) -> String {
        // Strip leading paths to show only project-relative path
        let parts: Vec<&str> = path.split('/').collect();

        // If the path contains "projs/dumpfs", extract everything after that
        let mut rel_path = path.to_string();
        if let Some(pos) = path.find("projs/dumpfs") {
            if let Some(p) = path.get(pos + "projs/dumpfs".len() + 1..) {
                rel_path = p.to_string();
            }
        }

        // If relative path is empty, use the original filename
        if rel_path.is_empty() && !parts.is_empty() {
            rel_path = parts.last().unwrap_or(&"").to_string();
        }

        // Truncate if too long
        if rel_path.len() <= max_len {
            return rel_path;
        }

        // If too long, preserve the most meaningful part (filename and parent dirs)
        let parts: Vec<&str> = path.split('/').collect();
        if parts.len() <= 2 {
            return format!("...{}", &path[path.len().saturating_sub(max_len - 3)..]);
        }

        // Keep the last few segments
        let mut result = String::new();
        let mut current_len = 3; // Start with "..."
        let mut segments = Vec::new();

        for part in parts.iter().rev() {
            let part_len = part.len() + 1; // +1 for '/'
            if current_len + part_len <= max_len {
                segments.push(*part);
                current_len += part_len;
            } else {
                break;
            }
        }

        result.push_str("...");
        for part in segments.iter().rev() {
            result.push('/');
            result.push_str(part);
        }

        result
    }

    // Create a summary table using the tabled crate
    fn create_summary_table(&self, report: &ScanReport) -> String {
        // Define the summary table data structure
        #[derive(Tabled)]
        struct SummaryRow {
            #[tabled(rename = "Metric")]
            key: String,

            #[tabled(rename = "Value")]
            value: String,
        }

        let mut rows = Vec::new();

        // Add rows to the summary table
        rows.push(SummaryRow {
            key: "ðŸ“‚ Output File".to_string(),
            value: report.output_file.clone(),
        });

        rows.push(SummaryRow {
            key: "â±ï¸ Process Time".to_string(),
            value: format!("{:.4?}", report.duration),
        });

        rows.push(SummaryRow {
            key: "ðŸ“„ Files Processed".to_string(),
            value: self.format_number(report.files_processed),
        });

        rows.push(SummaryRow {
            key: "ðŸ“ Total Lines".to_string(),
            value: self.format_number(report.total_lines),
        });

        // Use actual token count if available, otherwise use estimate
        let token_text = if let Some(tokens) = report.total_tokens {
            format!("{} tokens (counted)", self.format_number(tokens))
        } else {
            let estimated_tokens = report.total_chars / 4;
            format!(
                "{} tokens (estimated)",
                self.format_number(estimated_tokens)
            )
        };

        rows.push(SummaryRow {
            key: "ðŸ“¦ LLM Tokens".to_string(),
            value: token_text,
        });

        // Add cache statistics if available
        if let (Some(hits), Some(misses)) = (report.token_cache_hits, report.token_cache_misses) {
            let total = hits + misses;
            let hit_rate = if total > 0 {
                format!("{:.1}%", (hits as f64 / total as f64) * 100.0)
            } else {
                "0.0%".to_string()
            };

            rows.push(SummaryRow {
                key: "ðŸ”„ Cache Hit Rate".to_string(),
                value: format!("{} ({} hits / {} total)", hit_rate, hits, total),
            });
        }

        // Create and style the table
        let mut table = Table::new(rows);
        table
            .with(Style::rounded())
            .with(Padding::new(1, 1, 0, 0))
            .with(Modify::new(Columns::new(..)).with(Alignment::left()));

        table.to_string()
    }

    // Create a files table using the tabled crate
    fn create_files_table(&self, report: &ScanReport) -> String {
        // Define the files table data structure
        #[derive(Tabled)]
        struct FileRow {
            #[tabled(rename = "File Path")]
            path: String,

            #[tabled(rename = "Lines")]
            lines: String,

            #[tabled(rename = "Est. Tokens")]
            tokens: String,
        }

        // Sort files by character count
        let mut files: Vec<_> = report.file_details.iter().collect();
        files.sort_by(|(_, a), (_, b)| b.chars.cmp(&a.chars));

        // Determine if we show all files or just top 10
        let files_to_show = if report.file_details.len() > 15 {
            &files[0..10]
        } else {
            &files[..]
        };

        // Generate rows for the table
        let rows: Vec<FileRow> = files_to_show
            .iter()
            .map(|(path, info)| {
                // Format and truncate path if needed
                let display_path = self.format_path(path, 60);

                // Use actual token count if available, otherwise estimate
                let token_count = if let Some(tokens) = info.tokens {
                    self.format_number(tokens)
                } else {
                    let estimated_tokens = info.chars / 4;
                    self.format_number(estimated_tokens)
                };

                FileRow {
                    path: display_path,
                    lines: self.format_number(info.lines),
                    tokens: token_count,
                }
            })
            .collect();

        // Create and style the table
        let mut table = Table::new(rows);
        table
            .with(Style::rounded())
            .with(Padding::new(1, 1, 0, 0))
            .with(Modify::new(Columns::new(..)).with(Alignment::left()));

        table.to_string()
    }

    // Generate a console table report
    fn generate_console_report(&self, report: &ScanReport) -> String {
        // Generate summary and files tables
        let summary_table = self.create_summary_table(report);
        let files_table = self.create_files_table(report);

        // Create proper section titles
        let summary_title = "âœ…  EXTRACTION COMPLETE";
        let files_title = if report.file_details.len() > 15 {
            "ðŸ“‹  TOP 10 LARGEST FILES BY CHARACTER COUNT  ðŸ“‹"
        } else {
            "ðŸ“‹  PROCESSED FILES"
        };

        // Combine them with appropriate spacing and titles, but put files first
        format!(
            "{}\n{}\n\n{}\n{}",
            files_title, files_table, summary_title, summary_table
        )
    }
}

```

================================================
/home/user/projs/dumpfs/src/tests.rs
================================================

```rs
/*!
 * Tests for DumpFS functionality
 */

use std::fs::{self, File};
use std::io::{self, Write};
use std::path::{Path, PathBuf};
use std::sync::Arc;

use indicatif::ProgressBar;
use quick_xml::events::Event;
use quick_xml::Reader;
use tempfile::tempdir;

use crate::config::{Config, GitCachePolicy};
use crate::git::{GitHost, GitRepoInfo};
// Git module imports not needed as tests are moved
use crate::scanner::Scanner;
use crate::writer::XmlWriter;

// Helper function to create a test directory structure
fn setup_test_directory() -> io::Result<tempfile::TempDir> {
    let temp_dir = tempdir()?;

    // Create a simple directory structure
    fs::create_dir(temp_dir.path().join("dir1"))?;
    fs::create_dir(temp_dir.path().join("dir2"))?;
    fs::create_dir(temp_dir.path().join("dir1").join("subdir"))?;

    // Create text files
    let mut file1 = File::create(temp_dir.path().join("file1.txt"))?;
    writeln!(file1, "This is a text file with content")?;

    let mut file2 = File::create(temp_dir.path().join("dir1").join("file2.txt"))?;
    writeln!(file2, "This is another text file\nwith multiple lines")?;

    let mut file3 = File::create(
        temp_dir
            .path()
            .join("dir1")
            .join("subdir")
            .join("file3.txt"),
    )?;
    writeln!(file3, "Nested file content")?;

    // Create files to be ignored
    fs::create_dir(temp_dir.path().join(".git"))?;
    let mut git_file = File::create(temp_dir.path().join(".git").join("config"))?;
    writeln!(git_file, "[core]\n\trepositoryformatversion = 0")?;

    // Create a binary file
    let mut bin_file = File::create(temp_dir.path().join("binary.bin"))?;
    bin_file.write_all(&[0u8, 1u8, 2u8, 3u8])?;

    // Create a symlink if not on Windows
    #[cfg(not(target_os = "windows"))]
    std::os::unix::fs::symlink(
        temp_dir.path().join("file1.txt"),
        temp_dir.path().join("symlink.txt"),
    )?;

    Ok(temp_dir)
}

// Helper function to create a test directory with a .gitignore file
fn setup_gitignore_test_directory() -> io::Result<tempfile::TempDir> {
    let temp_dir = setup_test_directory()?;

    // Create a .gitignore file
    let mut gitignore = File::create(temp_dir.path().join(".gitignore"))?;
    writeln!(gitignore, "# Ignore all .txt files")?;
    writeln!(gitignore, "*.txt")?;
    writeln!(gitignore, "# Ignore binary.bin")?;
    writeln!(gitignore, "binary.bin")?;

    // Create some additional files that aren't explicitly ignored
    let mut not_ignored = File::create(temp_dir.path().join("not_ignored.md"))?;
    writeln!(not_ignored, "# This file shouldn't be ignored")?;

    Ok(temp_dir)
}

// Helper function to create a large file (>1MB)
fn create_large_file(dir: &Path) -> io::Result<()> {
    let path = dir.join("large_file.txt");
    let mut file = File::create(path)?;

    // Write over 1MB of data
    let line = "This is a line of text that will be repeated many times to create a large file.\n";
    for _ in 0..20000 {
        file.write_all(line.as_bytes())?;
    }

    Ok(())
}

// Test basic scanning functionality
#[test]
fn test_basic_scan() -> io::Result<()> {
    let temp_dir = setup_test_directory()?;
    let output_file = temp_dir.path().join("output.xml");

    let config = Config {
        target_dir: temp_dir.path().to_path_buf(),
        output_file: output_file.clone(),
        ignore_patterns: vec![],
        include_patterns: vec![],
        num_threads: 1,
        respect_gitignore: false,
        gitignore_path: None,
        model: None,
        repo_url: None,
        git_repo: None,
        git_cache_policy: GitCachePolicy::AlwaysPull,
        include_metadata: false,
        clip: false,
    };

    let progress = Arc::new(ProgressBar::hidden());
    let scanner = Scanner::new(config.clone(), Arc::clone(&progress));
    let writer = XmlWriter::new(config);

    let root_node = scanner.scan()?;
    writer.write(&root_node)?;

    // Check that the output file exists
    assert!(output_file.exists());

    // Read the XML file to verify structure
    let xml_content = fs::read_to_string(&output_file)?;

    // Check basic structure
    assert!(xml_content.contains("<directory_scan"));
    assert!(xml_content.contains("<system_info>"));
    assert!(xml_content.contains("<hostname>"));
    assert!(xml_content.contains("<directory name="));
    assert!(xml_content.contains("<file name=\"file1.txt\""));
    assert!(xml_content.contains("This is a text file with content"));

    // The .git directory should be ignored by default
    assert!(!xml_content.contains(".git"));

    Ok(())
}

// Test ignore patterns
#[test]
fn test_ignore_patterns() -> io::Result<()> {
    let temp_dir = setup_test_directory()?;
    let output_file = temp_dir.path().join("output.xml");

    let config = Config {
        target_dir: temp_dir.path().to_path_buf(),
        output_file: output_file.clone(),
        ignore_patterns: vec!["*.txt".to_string()],
        include_patterns: vec![],
        num_threads: 1,
        respect_gitignore: false,
        model: None,
        gitignore_path: None,
        repo_url: None,
        git_repo: None,
        git_cache_policy: GitCachePolicy::AlwaysPull,
        include_metadata: false,
        clip: false,
    };

    let progress = Arc::new(ProgressBar::hidden());
    let scanner = Scanner::new(config.clone(), Arc::clone(&progress));
    let writer = XmlWriter::new(config);

    let root_node = scanner.scan()?;
    writer.write(&root_node)?;

    // Read the XML file
    let xml_content = fs::read_to_string(&output_file)?;

    // All .txt files should be ignored
    assert!(!xml_content.contains("file1.txt"));
    assert!(!xml_content.contains("file2.txt"));
    assert!(!xml_content.contains("file3.txt"));

    // The binary file should still be included
    assert!(xml_content.contains("binary.bin"));

    Ok(())
}

// Test include patterns
#[test]
fn test_include_patterns() -> io::Result<()> {
    let temp_dir = setup_test_directory()?;
    let output_file = temp_dir.path().join("output.xml");

    let config = Config {
        target_dir: temp_dir.path().to_path_buf(),
        output_file: output_file.clone(),
        ignore_patterns: vec![],
        include_patterns: vec!["*.bin".to_string()],
        num_threads: 1,
        respect_gitignore: false,
        model: None,
        gitignore_path: None,
        repo_url: None,
        git_repo: None,
        git_cache_policy: GitCachePolicy::AlwaysPull,
        include_metadata: false,
        clip: false,
    };

    let progress = Arc::new(ProgressBar::hidden());
    let scanner = Scanner::new(config.clone(), Arc::clone(&progress));
    let writer = XmlWriter::new(config);

    let root_node = scanner.scan()?;
    writer.write(&root_node)?;

    // Read the XML file
    let xml_content = fs::read_to_string(&output_file)?;

    // Only .bin files should be included
    assert!(!xml_content.contains("file1.txt"));
    assert!(!xml_content.contains("file2.txt"));
    assert!(!xml_content.contains("file3.txt"));
    assert!(xml_content.contains("binary.bin"));

    Ok(())
}

// Test handling of large files
#[test]
fn test_large_file_handling() -> io::Result<()> {
    let temp_dir = setup_test_directory()?;
    create_large_file(temp_dir.path())?;

    let output_file = temp_dir.path().join("output.xml");

    let config = Config {
        target_dir: temp_dir.path().to_path_buf(),
        output_file: output_file.clone(),
        ignore_patterns: vec![],
        include_patterns: vec![],
        num_threads: 1,
        respect_gitignore: false,
        model: None,
        gitignore_path: None,
        repo_url: None,
        git_repo: None,
        git_cache_policy: GitCachePolicy::AlwaysPull,
        include_metadata: false,
        clip: false,
    };

    let progress = Arc::new(ProgressBar::hidden());
    let scanner = Scanner::new(config.clone(), Arc::clone(&progress));
    let writer = XmlWriter::new(config);

    let root_node = scanner.scan()?;
    writer.write(&root_node)?;

    // Read the XML file
    let xml_content = fs::read_to_string(&output_file)?;

    // Large file should be mentioned but content should be truncated
    assert!(xml_content.contains("large_file.txt"));
    assert!(xml_content.contains("File too large to include content"));

    Ok(())
}

// Test XML structure validity
#[test]
fn test_xml_validity() -> io::Result<()> {
    let temp_dir = setup_test_directory()?;
    let output_file = temp_dir.path().join("output.xml");

    let config = Config {
        target_dir: temp_dir.path().to_path_buf(),
        output_file: output_file.clone(),
        ignore_patterns: vec![],
        include_patterns: vec![],
        num_threads: 1,
        model: None,
        respect_gitignore: false,
        gitignore_path: None,
        repo_url: None,
        git_repo: None,
        git_cache_policy: GitCachePolicy::AlwaysPull,
        include_metadata: false,
        clip: false,
    };

    let progress = Arc::new(ProgressBar::hidden());
    let scanner = Scanner::new(config.clone(), Arc::clone(&progress));
    let writer = XmlWriter::new(config);

    let root_node = scanner.scan()?;
    writer.write(&root_node)?;

    // Parse the XML file to verify it's well-formed
    let file_content = fs::read_to_string(&output_file)?;
    let mut reader = Reader::from_str(&file_content);

    let mut depth = 0;
    let mut buf = Vec::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(_)) => depth += 1,
            Ok(Event::End(_)) => depth -= 1,
            Ok(Event::Eof) => break,
            Err(e) => panic!("Error parsing XML: {}", e),
            _ => (),
        }
        buf.clear();
    }

    // If XML is well-formed, depth should be 0 at the end
    assert_eq!(depth, 0, "XML structure is not well-balanced");

    Ok(())
}

// Test respecting .gitignore files
#[test]
fn test_respect_gitignore() -> io::Result<()> {
    let temp_dir = setup_gitignore_test_directory()?;
    let output_file = temp_dir.path().join("output.xml");

    let config = Config {
        target_dir: temp_dir.path().to_path_buf(),
        output_file: output_file.clone(),
        ignore_patterns: vec![],
        include_patterns: vec![],
        num_threads: 1,
        respect_gitignore: true,
        model: None,
        gitignore_path: None,
        repo_url: None,
        git_repo: None,
        git_cache_policy: GitCachePolicy::AlwaysPull,
        include_metadata: false,
        clip: false,
    };

    let progress = Arc::new(ProgressBar::hidden());
    let scanner = Scanner::new(config.clone(), Arc::clone(&progress));
    let writer = XmlWriter::new(config);

    let root_node = scanner.scan()?;
    writer.write(&root_node)?;

    // Read the XML file
    let xml_content = fs::read_to_string(&output_file)?;

    // Files excluded by .gitignore should not be present
    assert!(!xml_content.contains("file1.txt"));
    assert!(!xml_content.contains("file2.txt"));
    assert!(!xml_content.contains("file3.txt"));
    assert!(!xml_content.contains("binary.bin"));

    // Files not excluded by .gitignore should be present
    assert!(xml_content.contains("not_ignored.md"));

    Ok(())
}

#[test]
fn test_output_file_path_for_git_repo() {
    // Create a mock GitRepoInfo
    let repo_path = PathBuf::from("/tmp/cache/dumpfs/github/username/repo");
    let git_repo = GitRepoInfo {
        url: "https://github.com/username/repo".to_string(),
        host: GitHost::GitHub,
        owner: "username".to_string(),
        name: "repo".to_string(),
        cache_path: repo_path.clone(),
    };

    // Test cases for output file paths
    let test_cases = vec![
        // Relative file with no path component -> save to repo dir
        (".dumpfs.context.xml", repo_path.join(".dumpfs.context.xml")),
        // Relative file with path component -> keep as is
        ("output/file.xml", PathBuf::from("output/file.xml")),
        // Absolute path -> keep as is
        ("/tmp/output.xml", PathBuf::from("/tmp/output.xml")),
    ];

    for (input, expected) in test_cases {
        // Create a config with the test input
        let mut config = Config {
            target_dir: repo_path.clone(),
            output_file: PathBuf::from(input),
            ignore_patterns: vec![],
            include_patterns: vec![],
            num_threads: 1,
            respect_gitignore: false,
            gitignore_path: None,
            model: None,
            repo_url: Some("https://github.com/username/repo".to_string()),
            git_repo: Some(git_repo.clone()),
            git_cache_policy: GitCachePolicy::AlwaysPull,
            include_metadata: false,
            clip: false,
        };

        // Apply output file path logic (simplified from main.rs)
        if let Some(repo) = &config.git_repo {
            let output_path = PathBuf::from(input);
            if !output_path.is_absolute()
                && (output_path.parent().is_none()
                    || output_path
                        .parent()
                        .expect("Parent should be Some if not None")
                        == Path::new(""))
            {
                config.output_file = repo.cache_path.join(output_path);
            }
        }

        // Check if the path was adjusted correctly
        assert_eq!(config.output_file, expected);
    }
}

```

================================================
/home/user/projs/dumpfs/src/scanner.rs
================================================

```rs
/*!
 * Directory and file scanning functionality
 */

use std::collections::HashMap;
use std::fs::{self, File};
use std::io::{BufRead, BufReader, Read};
use std::os::unix::fs::PermissionsExt;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};

use glob_match::glob_match;
use ignore::{DirEntry as IgnoreDirEntry, WalkBuilder};
use indicatif::ProgressBar;
use rayon::prelude::*;
use walkdir::{DirEntry, WalkDir};

use crate::config::Config;
use crate::error::{DumpFsError, Result, ResultExt};
use crate::types::{BinaryNode, DirectoryNode, FileNode, FileType, Metadata, Node, SymlinkNode};
use crate::utils::{format_file_size, DEFAULT_IGNORE};

use crate::report::FileReportInfo;
use crate::tokenizer::{create_tokenizer, get_global_cache_stats, Tokenizer};

/// Scanner statistics
#[derive(Debug, Clone, Default)]
pub struct ScannerStatistics {
    /// Number of files processed
    pub files_processed: usize,
    /// Total number of lines
    pub total_lines: usize,
    /// Total number of characters
    pub total_chars: usize,
    /// Total number of tokens (if tokenizer is enabled)
    pub total_tokens: Option<usize>,
    /// Details for each file
    pub file_details: HashMap<String, FileReportInfo>,
    /// Token cache hits (if tokenizer caching is enabled)
    pub token_cache_hits: Option<usize>,
    /// Token cache misses (if tokenizer caching is enabled)
    pub token_cache_misses: Option<usize>,
}

/// Scanner for directory contents
pub struct Scanner {
    /// Scanner configuration
    config: Config,
    /// Progress bar
    pub progress: Arc<ProgressBar>,
    /// Scanner statistics
    statistics: Arc<Mutex<ScannerStatistics>>,
    /// Tokenizer (if enabled)
    tokenizer: Option<Box<dyn Tokenizer>>,
}

impl Scanner {
    /// Create a new scanner
    pub fn new(config: Config, progress: Arc<ProgressBar>) -> Self {
        // Create tokenizer if model is specified
        let tokenizer = if let Some(model) = config.model {
            let project_dir = config.target_dir.to_string_lossy().to_string();
            match create_tokenizer(model, &project_dir) {
                Ok(t) => {
                    progress.set_message(format!("Using tokenizer for model: {model:?}"));
                    Some(t)
                }
                Err(e) => {
                    eprintln!("Error creating tokenizer: {}", e);
                    None
                }
            }
        } else {
            None
        };

        Self {
            config,
            progress,
            statistics: Arc::new(Mutex::new(ScannerStatistics::default())),
            tokenizer,
        }
    }

    /// Normalize a path to be relative to the repository root
    pub fn normalize_path(&self, path: &Path) -> PathBuf {
        // If we have git repo info, make paths relative to repo root
        if let Some(repo_info) = &self.config.git_repo {
            // Try to make the path relative to the repository cache path
            if let Ok(rel_path) = path.strip_prefix(&repo_info.cache_path) {
                return if rel_path == Path::new("") {
                    // If it's the root, just return an empty path
                    PathBuf::new()
                } else {
                    // Otherwise, return the relative path
                    rel_path.to_path_buf()
                };
            }
        }

        // Default case: use the path as is
        path.to_path_buf()
    }

    /// Convert an absolute path to a normalized relative path for reporting
    pub fn get_normalized_path_for_reporting(&self, abs_path: &Path) -> String {
        if let Some(repo_info) = &self.config.git_repo {
            // For git repos, use owner/repo/path format
            if let Ok(rel_path) = abs_path.strip_prefix(&repo_info.cache_path) {
                // If it's a directory with no path components, just return owner/repo
                if rel_path == Path::new("") {
                    format!("{}/{}", repo_info.owner, repo_info.name)
                } else {
                    format!(
                        "{}/{}/{}",
                        repo_info.owner,
                        repo_info.name,
                        rel_path.display()
                    )
                }
            } else {
                // Fallback to full path
                abs_path.display().to_string()
            }
        } else {
            // For local paths, just use the path as is
            abs_path.display().to_string()
        }
    }

    /// Get scanner statistics
    pub fn get_statistics(&self) -> Result<ScannerStatistics> {
        let mut stats = self
            .statistics
            .lock()
            .map_err(|_| {
                DumpFsError::Unexpected("Failed to acquire lock on statistics".to_string())
            })?
            .clone();

        // If we have a tokenizer, get cache stats from global counters
        if self.tokenizer.is_some() {
            let cache_stats = get_global_cache_stats();
            stats.token_cache_hits = Some(cache_stats.hits);
            stats.token_cache_misses = Some(cache_stats.misses);
        }

        Ok(stats)
    }

    /// Scan the target directory and return the directory tree
    pub fn scan(&self) -> Result<DirectoryNode> {
        let abs_path = fs::canonicalize(&self.config.target_dir).with_context(|| {
            format!(
                "Failed to canonicalize path: {}",
                self.config.target_dir.display()
            )
        })?;

        // Determine the base directory name and path
        let dir_name = if let Some(repo_info) = &self.config.git_repo {
            // For git repos, use the repo name
            repo_info.name.clone()
        } else {
            // For local directories, use the directory name
            abs_path
                .file_name()
                .ok_or_else(|| {
                    DumpFsError::PathNotFound(format!(
                        "No file name in path: {}",
                        abs_path.display()
                    ))
                })?
                .to_string_lossy()
                .to_string()
        };

        // Create the initial relative path
        let rel_path = PathBuf::from(&dir_name);

        self.scan_directory(&abs_path, &rel_path)
    }

    /// Scan a directory and return its node representation
    fn scan_directory(&self, abs_path: &Path, rel_path: &Path) -> Result<DirectoryNode> {
        let metadata = self.get_metadata(abs_path).with_context(|| {
            format!(
                "Failed to get metadata for directory: {}",
                abs_path.display()
            )
        })?;
        let mut contents = Vec::new();

        // Determine which entries to process based on whether we're using gitignore
        if self.config.respect_gitignore {
            // Use ignore crate's Walk to handle .gitignore patterns
            let mut walker = WalkBuilder::new(abs_path);
            walker.max_depth(Some(1)); // Limit depth to just the current directory

            // Use custom gitignore file if specified
            if let Some(gitignore_path) = &self.config.gitignore_path {
                walker.add_custom_ignore_filename(gitignore_path);
            }

            // Get all entries using the ignore walker
            let entries: Vec<IgnoreDirEntry> = walker
                .build()
                .filter_map(|entry_result| entry_result.ok()) // Use closure instead of Result::ok to resolve type issues
                .filter(|e| e.path() != abs_path) // Skip the root directory itself
                .filter(|e| !self.should_ignore(e.path()))
                .filter(|e| self.should_include(e.path()))
                .collect();

            // Split into directories and files
            let (dirs, files): (Vec<_>, Vec<_>) =
                entries.into_iter().partition(|e| e.path().is_dir());

            // Process directories first (sequential)
            for entry in dirs {
                let entry_path = entry.path();
                // Use normalize_path to get the correct relative path
                let normalized_path = self.normalize_path(entry_path);
                let new_rel_path = if normalized_path.components().count() > 0 {
                    // If we have a normalized path, use it
                    normalized_path
                } else {
                    // Otherwise, just join with the entry name
                    let entry_name = entry_path
                        .file_name()
                        .ok_or_else(|| {
                            DumpFsError::PathNotFound(format!(
                                "No file name in path: {}",
                                entry_path.display()
                            ))
                        })?
                        .to_string_lossy()
                        .to_string();
                    rel_path.join(&entry_name)
                };

                match self.scan_directory(entry_path, &new_rel_path) {
                    Ok(dir_node) => contents.push(Node::Directory(dir_node)),
                    Err(e) => {
                        eprintln!("Error processing directory {}: {}", entry_path.display(), e)
                    }
                }
            }

            // Process files in parallel
            let file_nodes: Vec<Node> = files
                .par_iter()
                .filter_map(|entry: &IgnoreDirEntry| {
                    let entry_path = entry.path();
                    let entry_name = match entry_path.file_name() {
                        Some(name) => name.to_string_lossy().to_string(),
                        None => {
                            eprintln!("Error: No file name in path: {}", entry_path.display());
                            return None;
                        }
                    };
                    let new_rel_path = rel_path.join(&entry_name);

                    match self.process_file(entry_path, &new_rel_path) {
                        Ok(node) => Some(node),
                        Err(e) => {
                            eprintln!("Error processing {}: {}", entry_path.display(), e);
                            None
                        }
                    }
                })
                .collect();

            contents.extend(file_nodes);
        } else {
            // Use traditional walkdir approach when not respecting .gitignore
            let entries: Vec<DirEntry> = WalkDir::new(abs_path)
                .max_depth(1)
                .min_depth(1)
                .into_iter()
                .filter_map(|entry_result| entry_result.ok()) // Use closure instead of Result::ok to resolve type issues
                .filter(|e| !self.should_ignore(e.path()))
                .filter(|e| self.should_include(e.path()))
                .collect();

            // Split into directories and files
            let (dirs, files): (Vec<_>, Vec<_>) =
                entries.into_iter().partition(|e| e.file_type().is_dir());

            // Process directories first (sequential)
            for entry in dirs {
                let entry_name = entry.file_name().to_string_lossy().to_string();
                let new_rel_path = rel_path.join(&entry_name);

                match self.scan_directory(entry.path(), &new_rel_path) {
                    Ok(dir_node) => contents.push(Node::Directory(dir_node)),
                    Err(e) => eprintln!(
                        "Error processing directory {}: {}",
                        entry.path().display(),
                        e
                    ),
                }
            }

            // Process files in parallel
            let file_nodes: Vec<Node> = files
                .par_iter()
                .filter_map(|entry: &DirEntry| {
                    let entry_name = entry.file_name().to_string_lossy().to_string();
                    let new_rel_path = rel_path.join(&entry_name);

                    match self.process_file(entry.path(), &new_rel_path) {
                        Ok(node) => Some(node),
                        Err(e) => {
                            eprintln!("Error processing {}: {}", entry.path().display(), e);
                            None
                        }
                    }
                })
                .collect();

            contents.extend(file_nodes);
        }

        let name = abs_path
            .file_name()
            .ok_or_else(|| {
                DumpFsError::PathNotFound(format!("No file name in path: {}", abs_path.display()))
            })?
            .to_string_lossy()
            .to_string();

        Ok(DirectoryNode {
            name,
            path: rel_path.to_path_buf(),
            metadata,
            contents,
        })
    }

    /// Process a single file and return its node representation
    fn process_file(&self, abs_path: &Path, rel_path: &Path) -> Result<Node> {
        self.progress.inc(1);

        // Update progress message to show current file
        let file_name = abs_path
            .file_name()
            .ok_or_else(|| {
                DumpFsError::PathNotFound(format!("No file name in path: {}", abs_path.display()))
            })?
            .to_string_lossy()
            .to_string();

        // Update the progress message with the filename
        // Truncate if too long to avoid display issues
        let display_name = if file_name.len() > 40 {
            format!("...{}", &file_name[file_name.len().saturating_sub(37)..])
        } else {
            file_name.clone()
        };

        // Enhance display with repository context if applicable
        let progress_message = if let Some(repo_info) = &self.config.git_repo {
            format!(
                "Current file: {}/{}/{}",
                repo_info.owner, repo_info.name, display_name
            )
        } else {
            format!("Current file: {}", display_name)
        };

        self.progress.set_message(progress_message);

        let file_type = self
            .get_file_type(abs_path)
            .with_context(|| format!("Failed to determine file type for {}", abs_path.display()))?;
        let metadata = self
            .get_metadata(abs_path)
            .with_context(|| format!("Failed to get metadata for {}", abs_path.display()))?;

        // Use the normalized path for reporting
        let file_path = if let Some(repo_info) = &self.config.git_repo {
            // For repositories, use the format owner/repo/path
            format!(
                "{}/{}/{}",
                repo_info.owner,
                repo_info.name,
                rel_path.display()
            )
        } else {
            // For local directories, use the relative path as is
            rel_path.to_string_lossy().to_string()
        };

        match file_type {
            FileType::TextFile => {
                let content = self
                    .read_file_content(abs_path)
                    .with_context(|| format!("Failed to read content of {}", abs_path.display()))?;
                Ok(Node::File(FileNode {
                    name: file_name,
                    path: rel_path.to_path_buf(),
                    metadata,
                    content,
                }))
            }
            FileType::BinaryFile => {
                // Update statistics for binary files
                {
                    let mut stats = self.statistics.lock().map_err(|_| {
                        DumpFsError::Unexpected("Failed to acquire lock on statistics".to_string())
                    })?;
                    stats.files_processed += 1;
                    stats.file_details.insert(
                        file_path,
                        FileReportInfo {
                            lines: 0,
                            chars: 0,
                            tokens: None,
                        },
                    );
                }

                Ok(Node::Binary(BinaryNode {
                    name: file_name,
                    path: rel_path.to_path_buf(),
                    metadata,
                }))
            }
            FileType::Symlink => {
                let target = fs::read_link(abs_path)
                    .with_context(|| {
                        format!("Failed to read symlink target for {}", abs_path.display())
                    })?
                    .to_string_lossy()
                    .to_string();

                // Update statistics for symlinks
                {
                    let mut stats = self.statistics.lock().map_err(|_| {
                        DumpFsError::Unexpected("Failed to acquire lock on statistics".to_string())
                    })?;
                    stats.files_processed += 1;
                    stats.file_details.insert(
                        file_path,
                        FileReportInfo {
                            lines: 0,
                            chars: target.chars().count(),
                            tokens: None,
                        },
                    );
                }

                Ok(Node::Symlink(SymlinkNode {
                    name: file_name,
                    path: rel_path.to_path_buf(),
                    metadata,
                    target,
                }))
            }
            _ => Err(DumpFsError::Unexpected(format!(
                "Unexpected file type for {}",
                abs_path.display()
            ))),
        }
    }

    /// Check if a file should be ignored based on patterns and defaults
    pub fn should_ignore(&self, path: &Path) -> bool {
        let file_name = match path.file_name() {
            Some(name) => name.to_string_lossy(),
            None => return true, // If there's no filename, ignore it
        };

        // Check custom ignore patterns
        for pattern in &self.config.ignore_patterns {
            if glob_match(pattern, &file_name) {
                return true;
            }
        }

        // Check default ignore patterns
        if DEFAULT_IGNORE.iter().any(|&p| p == file_name) {
            return true;
        }

        // Don't process the output file itself
        if path.ends_with(&self.config.output_file) {
            return true;
        }

        false
    }

    /// Check if a file should be included based on patterns
    pub fn should_include(&self, path: &Path) -> bool {
        // If no include patterns, include everything
        if self.config.include_patterns.is_empty() {
            return true;
        }

        let file_name = match path.file_name() {
            Some(name) => name.to_string_lossy(),
            None => return false, // If there's no filename, don't include it
        };

        // Check against include patterns
        for pattern in &self.config.include_patterns {
            if glob_match(pattern, &file_name) {
                return true;
            }
        }

        false
    }

    /// Determine the type of a file
    fn get_file_type(&self, path: &Path) -> Result<FileType> {
        let metadata = fs::metadata(path).with_context(|| {
            format!(
                "Failed to get metadata for file type detection: {}",
                path.display()
            )
        })?;

        if metadata.is_dir() {
            return Ok(FileType::Directory);
        }

        if metadata.file_type().is_symlink() {
            return Ok(FileType::Symlink);
        }

        if metadata.is_file() {
            // For smaller files, try to detect if they're text
            if metadata.len() < 8_000_000 {
                // Read a sample of the file to determine type
                let mut buffer = vec![0; std::cmp::min(8192, metadata.len() as usize)];
                if !buffer.is_empty() {
                    let mut file = File::open(path).with_context(|| {
                        format!("Failed to open file for type detection: {}", path.display())
                    })?;
                    let bytes_read = file.read(&mut buffer).with_context(|| {
                        format!(
                            "Failed to read from file for type detection: {}",
                            path.display()
                        )
                    })?;
                    buffer.truncate(bytes_read);

                    // Simple heuristic for text files: check for valid UTF-8 and high text-to-binary ratio
                    if String::from_utf8(buffer.clone()).is_ok() {
                        // Count binary characters (0x00-0x08, 0x0E-0x1F)
                        let binary_count = buffer
                            .iter()
                            .filter(|&&b| (b < 9) || (b > 13 && b < 32))
                            .count();
                        let binary_ratio = binary_count as f32 / buffer.len() as f32;

                        if binary_ratio < 0.1 {
                            return Ok(FileType::TextFile);
                        }
                    }
                }
            }

            // Default to binary for any non-text file
            return Ok(FileType::BinaryFile);
        }

        Ok(FileType::Other)
    }

    /// Extract metadata from a file
    fn get_metadata(&self, path: &Path) -> Result<Metadata> {
        let fs_metadata = fs::metadata(path)
            .with_context(|| format!("Failed to get file metadata for {}", path.display()))?;

        let modified = fs_metadata
            .modified()
            .with_context(|| format!("Failed to get modified time for {}", path.display()))?;

        Ok(Metadata {
            size: fs_metadata.len(),
            modified,
            permissions: format!("{:o}", fs_metadata.permissions().mode() & 0o777),
        })
    }

    /// Read the content of a text file and update statistics
    fn read_file_content(&self, path: &Path) -> Result<Option<String>> {
        let metadata = fs::metadata(path).with_context(|| {
            format!(
                "Failed to get metadata for file content: {}",
                path.display()
            )
        })?;
        // Get the normalized path for reporting
        let file_path = self.get_normalized_path_for_reporting(path);

        // Skip large files
        if metadata.len() > 1_048_576 {
            // 1MB limit
            let message = format!(
                "File too large to include content. Size: {}",
                format_file_size(metadata.len())
            );

            // Still update statistics for skipped files
            {
                let mut stats = self.statistics.lock().map_err(|_| {
                    DumpFsError::Unexpected("Failed to acquire lock on statistics".to_string())
                })?;
                stats.files_processed += 1;
                stats.file_details.insert(
                    file_path,
                    FileReportInfo {
                        lines: 0,
                        chars: 0,
                        tokens: None,
                    },
                );
            }

            return Ok(Some(message));
        }

        // Read file content
        let mut content = String::new();
        match File::open(path) {
            Ok(file) => {
                let mut line_count = 0;
                let mut char_count = 0;

                // Count lines and chars
                let reader = BufReader::new(&file);
                for line in reader.lines() {
                    match line {
                        Ok(line) => {
                            line_count += 1;
                            char_count += line.chars().count();
                            // Add newline char that's stripped by lines() iterator
                            char_count += 1;
                        }
                        Err(_) => break,
                    }
                }

                // Re-read file for content
                let mut file = File::open(path).with_context(|| {
                    format!("Failed to re-open file for content: {}", path.display())
                })?;
                match file.read_to_string(&mut content) {
                    Ok(_) => {}
                    Err(e) => return Ok(Some(format!("Failed to read file content: {}", e))),
                }

                // Count tokens if tokenizer is enabled
                let token_count = if let Some(tokenizer) = &self.tokenizer {
                    match tokenizer.count_tokens(&content) {
                        Ok(count) => Some(count.tokens),
                        Err(e) => {
                            eprintln!("Error counting tokens for {}: {}", path.display(), e);
                            None
                        }
                    }
                } else {
                    None
                };

                // Update statistics
                {
                    let mut stats = self.statistics.lock().map_err(|_| {
                        DumpFsError::Unexpected("Failed to acquire lock on statistics".to_string())
                    })?;
                    stats.files_processed += 1;
                    stats.total_lines += line_count;
                    stats.total_chars += char_count;

                    // Update token count if available
                    if let Some(tokens) = token_count {
                        stats.total_tokens = Some(stats.total_tokens.unwrap_or(0) + tokens);
                    }

                    stats.file_details.insert(
                        file_path,
                        FileReportInfo {
                            lines: line_count,
                            chars: char_count,
                            tokens: token_count,
                        },
                    );
                }
            }
            Err(e) => {
                return Ok(Some(format!("Failed to open file: {}", e)));
            }
        }

        Ok(Some(content))
    }
}

#[cfg(test)]
mod tests {
    use std::path::PathBuf;
    use std::sync::Arc;

    use indicatif::ProgressBar;

    use crate::config::{Config, GitCachePolicy};
    use crate::git::{GitHost, GitRepoInfo};
    use crate::scanner::Scanner;

    #[test]
    fn test_normalize_path() {
        // Create a test config with a mock Git repository
        let repo_path = PathBuf::from("/tmp/cache/dumpfs/github/username/repo");
        let git_repo = GitRepoInfo {
            url: "https://github.com/username/repo".to_string(),
            host: GitHost::GitHub,
            owner: "username".to_string(),
            name: "repo".to_string(),
            cache_path: repo_path.clone(),
        };

        let config = Config {
            target_dir: repo_path.clone(),
            output_file: PathBuf::from("output.xml"),
            ignore_patterns: vec![],
            include_patterns: vec![],
            num_threads: 1,
            respect_gitignore: false,
            gitignore_path: None,
            model: None,
            repo_url: Some("https://github.com/username/repo".to_string()),
            git_repo: Some(git_repo),
            git_cache_policy: GitCachePolicy::AlwaysPull,
            include_metadata: false,
            clip: false,
        };

        let scanner = Scanner::new(config, Arc::new(ProgressBar::hidden()));

        // Test paths at various depths
        let test_cases = vec![
            // Path in repo root should normalize to empty path or just filename
            (repo_path.join("file.txt"), PathBuf::from("file.txt")),
            // Path in subdirectory should be relative to repo root
            (
                repo_path.join("src").join("main.rs"),
                PathBuf::from("src/main.rs"),
            ),
            // Path outside repo shouldn't change
            (
                PathBuf::from("/other/path/file.txt"),
                PathBuf::from("/other/path/file.txt"),
            ),
        ];

        for (input, expected) in test_cases {
            let normalized = scanner.normalize_path(&input);
            assert_eq!(normalized, expected);
        }
    }

    #[test]
    fn test_get_normalized_path_for_reporting() {
        // Create a test config with a mock Git repository
        let repo_path = PathBuf::from("/tmp/cache/dumpfs/github/username/repo");
        let git_repo = GitRepoInfo {
            url: "https://github.com/username/repo".to_string(),
            host: GitHost::GitHub,
            owner: "username".to_string(),
            name: "repo".to_string(),
            cache_path: repo_path.clone(),
        };

        let config = Config {
            target_dir: repo_path.clone(),
            output_file: PathBuf::from("output.xml"),
            ignore_patterns: vec![],
            include_patterns: vec![],
            num_threads: 1,
            respect_gitignore: false,
            gitignore_path: None,
            model: None,
            repo_url: Some("https://github.com/username/repo".to_string()),
            git_repo: Some(git_repo),
            git_cache_policy: GitCachePolicy::AlwaysPull,
            include_metadata: false,
            clip: false,
        };

        let scanner = Scanner::new(config, Arc::new(ProgressBar::hidden()));

        // Test path formatting for different types of paths
        let root_path = repo_path.clone();
        let src_path = repo_path.join("src").join("main.rs");

        // Repository root should show as "username/repo"
        let root_display = scanner.get_normalized_path_for_reporting(&root_path);
        assert_eq!(root_display, "username/repo");

        // File in repo should show as "username/repo/src/main.rs"
        let src_display = scanner.get_normalized_path_for_reporting(&src_path);
        assert_eq!(src_display, "username/repo/src/main.rs");

        // Path outside repo should just use the full path
        let other_path = PathBuf::from("/other/path/file.txt");
        let other_display = scanner.get_normalized_path_for_reporting(&other_path);
        assert_eq!(other_display, "/other/path/file.txt");
    }
}

```

================================================
/home/user/projs/dumpfs/src/error.rs
================================================

```rs
//! Global error handling for dumpfs
//!
//! This module provides a centralized error type that can represent errors
//! from all modules in the project.

use std::io;
use thiserror::Error;

use crate::git::GitError;
use crate::tokenizer::TokenizerError;

/// Global error type for dumpfs operations
#[derive(Error, Debug)]
pub enum DumpFsError {
    /// Git-related errors
    #[error("Git error: {0}")]
    Git(#[from] GitError),

    /// Tokenizer-related errors
    #[error("Tokenizer error: {0}")]
    Tokenizer(#[from] TokenizerError),

    /// File system errors
    #[error("IO error: {0}")]
    Io(#[from] io::Error),

    /// Configuration errors
    #[error("Configuration error: {0}")]
    Config(String),

    /// XML processing errors
    #[error("XML error: {0}")]
    Xml(#[from] quick_xml::Error),

    /// JSON processing errors
    #[error("JSON error: {0}")]
    Json(#[from] serde_json::Error),

    /// Regular expression errors
    #[error("Regex error: {0}")]
    Regex(#[from] regex::Error),

    /// Scanner errors
    #[error("Scanner error: {0}")]
    Scanner(String),

    /// Writer errors
    #[error("Writer error: {0}")]
    Writer(String),

    /// Path not found
    #[error("Path not found: {0}")]
    PathNotFound(String),

    /// Invalid argument
    #[error("Invalid argument: {0}")]
    InvalidArgument(String),

    /// Unexpected error
    #[error("Unexpected error: {0}")]
    Unexpected(String),
}

/// Specialized Result type for dumpfs operations
pub type Result<T> = std::result::Result<T, DumpFsError>;

/// Creates a DumpFsError with a formatted message
#[macro_export]
macro_rules! error {
    ($error_type:ident, $($arg:tt)*) => {
        $crate::error::DumpFsError::$error_type(format!($($arg)*))
    };
}

/// Returns an error result with a formatted message
#[macro_export]
macro_rules! bail {
    ($error_type:ident, $($arg:tt)*) => {
        return Err($crate::error!($error_type, $($arg)*))
    };
}

/// Ensures a condition is true, otherwise returns an error
#[macro_export]
macro_rules! ensure {
    ($cond:expr, $error_type:ident, $($arg:tt)*) => {
        if !($cond) {
            $crate::bail!($error_type, $($arg)*)
        }
    };
}

/// Extension trait for adding context to errors
pub trait ResultExt<T, E> {
    /// Add additional context to an error
    fn with_context<C, F>(self, f: F) -> Result<T>
    where
        F: FnOnce() -> C,
        C: std::fmt::Display;
}

impl<T, E: std::error::Error + 'static> ResultExt<T, E> for std::result::Result<T, E> {
    fn with_context<C, F>(self, f: F) -> Result<T>
    where
        F: FnOnce() -> C,
        C: std::fmt::Display,
    {
        self.map_err(|e| {
            let context = f();
            DumpFsError::Unexpected(format!("{}: {}", context, e))
        })
    }
}

// Allow converting DumpFsError to io::Error for backward compatibility with tests
impl From<DumpFsError> for io::Error {
    fn from(err: DumpFsError) -> Self {
        io::Error::new(io::ErrorKind::Other, err.to_string())
    }
}

```

================================================
/home/user/projs/dumpfs/src/clipboard.rs
================================================

```rs
/*!
 * Clipboard support for DumpFS
 *
 * Provides functionality for copying output to system clipboard
 * with automatic detection of available clipboard mechanisms.
 */

use std::env;
use std::io::{self, Write};
use std::path::Path;
use std::process::{Command, Stdio};
use std::sync::OnceLock;

use thiserror::Error;

/// Error type for clipboard operations
#[derive(Error, Debug)]
pub enum ClipboardError {
    /// The command is not available on the system
    #[error("Command not found: {0}")]
    CommandNotFound(String),

    /// Failed to execute the command
    #[error("Command failed: {0}")]
    CommandFailed(String),

    /// No suitable clipboard mechanism was found
    #[error("No suitable clipboard mechanism found")]
    NoClipboardFound,

    /// IO error
    #[error("IO error: {0}")]
    Io(#[from] io::Error),
}

/// Result type for clipboard operations
pub type Result<T> = std::result::Result<T, ClipboardError>;

/// Trait for clipboard operations
pub trait Clipboard {
    /// Copy text to the clipboard
    fn copy_to_clipboard(&self, text: &str) -> Result<()>;
}

/// Available clipboard providers
#[derive(Debug, Clone, Copy)]
enum ClipboardProvider {
    /// tmux clipboard
    Tmux,
    /// X11 clipboard with xclip
    Xclip,
    /// X11 clipboard with xsel
    Xsel,
    /// Wayland clipboard
    Wayland,
    /// macOS clipboard
    MacOS,
    /// Windows clipboard (via WSL)
    Wsl,
    /// Termux clipboard
    Termux,
}

impl Clipboard for ClipboardProvider {
    fn copy_to_clipboard(&self, text: &str) -> Result<()> {
        let (cmd, args) = match self {
            Self::Tmux => {
                // Check tmux version to determine if we should use the -w flag
                let tmux_args = vec!["load-buffer", "-w", "-"];
                ("tmux", tmux_args)
            }
            Self::Xclip => ("xclip", vec!["-selection", "clipboard", "-in"]),
            Self::Xsel => ("xsel", vec!["-b", "-i"]),
            Self::Wayland => ("wl-copy", vec![]),
            Self::MacOS => ("pbcopy", vec![]),
            Self::Wsl => ("clip.exe", vec![]),
            Self::Termux => ("termux-clipboard-set", vec![]),
        };

        execute_clipboard_command(cmd, &args, text)
    }
}

//--------------------------------------------------------------------
// Public API
//--------------------------------------------------------------------

/// Copy text to the clipboard
///
/// Automatically detects the most appropriate clipboard mechanism
/// and uses it to copy text to the system clipboard.
///
/// # Arguments
/// * `text` - The text to copy to the clipboard
///
/// # Returns
/// * `Ok(())` - If the text was successfully copied
/// * `Err(ClipboardError)` - If the text could not be copied
///
/// # Examples
/// ```ignore
/// use dumpfs::clipboard::copy_to_clipboard;
///
/// let result = copy_to_clipboard("Hello, clipboard!");
/// if let Err(e) = result {
///     eprintln!("Failed to copy to clipboard: {}", e);
/// }
/// ```
pub fn copy_to_clipboard(text: &str) -> Result<()> {
    let clipboard = get_clipboard()?;
    clipboard.copy_to_clipboard(text)
}

/// Check if a command exists on the system
///
/// # Arguments
/// * `command` - The command to check
///
/// # Returns
/// * `true` - If the command exists and can be executed
/// * `false` - Otherwise
pub fn command_exists(command: &str) -> bool {
    // First check if the command exists in the PATH
    if let Ok(paths) = env::var("PATH") {
        for path in paths.split(':') {
            let p = Path::new(path).join(command);
            if p.exists() {
                return true;
            }
        }
    }

    // Try to run the command with '--version' flag as fallback
    Command::new(command)
        .arg("--version")
        .stdout(Stdio::null())
        .stderr(Stdio::null())
        .status()
        .is_ok()
}

//--------------------------------------------------------------------
// Internal Implementation
//--------------------------------------------------------------------

/// Get the appropriate clipboard implementation based on the system
fn get_clipboard() -> Result<Box<dyn Clipboard>> {
    // Define a list of providers to try in order of preference
    let providers = determine_clipboard_providers();

    // Try each provider in order
    for provider in providers {
        if let Some(clipboard) = try_clipboard_provider(provider) {
            return Ok(clipboard);
        }
    }

    // No clipboard mechanism found
    Err(ClipboardError::NoClipboardFound)
}

/// Execute a command to copy text to clipboard
///
/// This centralizes all the common command execution logic:
/// - Spawning the process
/// - Writing to stdin
/// - Waiting for completion
/// - Error handling
fn execute_clipboard_command(cmd: &str, args: &[&str], text: &str) -> Result<()> {
    let mut child = Command::new(cmd)
        .args(args)
        .stdin(Stdio::piped())
        .spawn()
        .map_err(|_| ClipboardError::CommandFailed(format!("Failed to spawn {}", cmd)))?;

    let stdin = child.stdin.as_mut().ok_or_else(|| {
        ClipboardError::CommandFailed(format!("Failed to open stdin for {}", cmd))
    })?;

    stdin
        .write_all(text.as_bytes())
        .map_err(|_| ClipboardError::CommandFailed(format!("Failed to write to {}", cmd)))?;

    let status = child
        .wait()
        .map_err(|_| ClipboardError::CommandFailed(format!("Failed to wait for {}", cmd)))?;

    if status.success() {
        Ok(())
    } else {
        Err(ClipboardError::CommandFailed(format!(
            "{} exited with status: {}",
            cmd, status
        )))
    }
}

/// Platform detection cache (using thread-safe lazy initialization)
static PLATFORM: OnceLock<&'static str> = OnceLock::new();

/// Determine the platform (cached)
fn get_platform() -> &'static str {
    PLATFORM.get_or_init(|| {
        if cfg!(target_os = "macos") {
            "macos"
        } else if cfg!(target_os = "windows") {
            "windows"
        } else if cfg!(target_os = "linux") {
            if env::var("WSL_DISTRO_NAME").is_ok() {
                "wsl"
            } else {
                "linux"
            }
        } else if cfg!(target_os = "android") {
            "android"
        } else {
            "unknown"
        }
    })
}

/// Determine which clipboard providers to try based on platform and preference
fn determine_clipboard_providers() -> Vec<ClipboardProvider> {
    let mut providers = Vec::with_capacity(3); // Pre-allocate space for typical number of providers

    // Always try tmux first if available and running (user preference)
    if command_exists("tmux") && is_tmux_running() {
        providers.push(ClipboardProvider::Tmux);
    }

    // Add platform-specific providers
    match get_platform() {
        "macos" => {
            if command_exists("pbcopy") {
                providers.push(ClipboardProvider::MacOS);
            }
        }
        "windows" | "wsl" => {
            if command_exists("clip.exe") {
                providers.push(ClipboardProvider::Wsl);
            }
        }
        "linux" => {
            // Try Wayland first
            if command_exists("wl-copy") {
                providers.push(ClipboardProvider::Wayland);
            }

            // Then X11 mechanisms
            if command_exists("xsel") {
                providers.push(ClipboardProvider::Xsel);
            }

            if command_exists("xclip") {
                providers.push(ClipboardProvider::Xclip);
            }
        }
        "android" => {
            if command_exists("termux-clipboard-set") {
                providers.push(ClipboardProvider::Termux);
            }
        }
        _ => {}
    }

    providers
}

/// Try to create a clipboard provider
fn try_clipboard_provider(provider: ClipboardProvider) -> Option<Box<dyn Clipboard>> {
    Some(Box::new(provider))
}

/// Check if tmux is running and available for clipboard operations
fn is_tmux_running() -> bool {
    // Check if TMUX environment variable is set (inside tmux session)
    if env::var("TMUX").is_ok() {
        return true;
    }

    // Try running tmux list-buffers as a fallback check
    let status = Command::new("tmux")
        .args(["list-buffers"])
        .stdout(Stdio::null())
        .stderr(Stdio::null())
        .status();

    // If the command succeeds, tmux is running and can be used
    status.map(|s| s.success()).unwrap_or(false)
}

//--------------------------------------------------------------------
// Tmux Version Handling
//--------------------------------------------------------------------

#[cfg(test)]
pub mod tests {
    use super::*;

    #[test]
    fn test_command_exists() {
        // These commands should exist on most systems
        assert!(command_exists("ls"));
        assert!(command_exists("echo"));

        // This command should not exist
        assert!(!command_exists("nonexistentcommandxyz"));
    }

    #[test]
    fn test_get_platform() {
        let platform = get_platform();

        // Just check that it's one of the known platforms
        assert!(
            platform == "macos"
                || platform == "windows"
                || platform == "wsl"
                || platform == "linux"
                || platform == "android"
                || platform == "unknown"
        );

        // Check that caching works (call again and verify it's the same result)
        let platform2 = get_platform();
        assert_eq!(platform, platform2);
    }

    #[test]
    #[ignore] // This test requires tmux to be installed and running
    fn test_tmux_clipboard() {
        // Skip the test if tmux is not available
        if !command_exists("tmux") {
            return;
        }

        // Check if we're in a tmux session
        let in_tmux = env::var("TMUX").is_ok();
        if !in_tmux {
            return;
        }

        let clipboard = ClipboardProvider::Tmux;
        let test_text = "Test text for tmux clipboard";

        // Copy to clipboard
        clipboard
            .copy_to_clipboard(test_text)
            .expect("Failed to copy to tmux clipboard");

        // Verify by reading from clipboard
        let output = Command::new("tmux")
            .args(["show-buffer"])
            .output()
            .expect("Failed to execute tmux show-buffer");

        let clipboard_content = String::from_utf8_lossy(&output.stdout);
        assert_eq!(clipboard_content.trim(), test_text);
    }
}

```

================================================
/home/user/projs/dumpfs/src/writer.rs
================================================

```rs
/*!
 * XML writer implementation for DumpFS
 */

use std::fs::File;
use std::io::{self, BufWriter, Write};
use std::path::PathBuf;

use chrono::Local;
use clap::ValueEnum;
use quick_xml::events::{BytesCData, BytesDecl, BytesEnd, BytesStart, BytesText, Event};

use crate::config::Config;
use crate::git::GitHost;
use crate::types::{BinaryNode, DirectoryNode, FileNode, Metadata, Node, SymlinkNode};

/// Enum for writer formats
#[derive(Default, Debug, Clone, ValueEnum)]
pub enum FsWriterFormatter {
    Xml,
    #[default]
    Txt,
}

impl FsWriterFormatter {
    pub fn write(&self, config: Config, root_node: &DirectoryNode) -> io::Result<()> {
        match self {
            FsWriterFormatter::Xml => XmlWriter::new(config).write(root_node),
            FsWriterFormatter::Txt => TxtWriter::new(config).write(root_node),
        }
    }
}

/// Trait for writing directory contents
trait Writer {
    fn write(&self, root_node: &DirectoryNode) -> io::Result<()>;
}

/// XML writer for directory contents
struct XmlWriter {
    config: Config,
}

impl XmlWriter {
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    fn write_system_info<W: Write>(&self, writer: &mut quick_xml::Writer<W>) -> io::Result<()> {
        writer.write_event(Event::Start(BytesStart::new("system_info")))?;

        // Write hostname
        writer.write_event(Event::Start(BytesStart::new("hostname")))?;
        let hostname = hostname::get()
            .map(|h| h.to_string_lossy().to_string())
            .unwrap_or_else(|_| "unknown".to_string());
        writer.write_event(Event::Text(BytesText::new(&hostname)))?;
        writer.write_event(Event::End(BytesEnd::new("hostname")))?;

        // Write OS
        writer.write_event(Event::Start(BytesStart::new("os")))?;
        let os = std::env::consts::OS;
        writer.write_event(Event::Text(BytesText::new(os)))?;
        writer.write_event(Event::End(BytesEnd::new("os")))?;

        // Write kernel version
        writer.write_event(Event::Start(BytesStart::new("kernel")))?;
        let kernel = std::env::consts::FAMILY;
        writer.write_event(Event::Text(BytesText::new(kernel)))?;
        writer.write_event(Event::End(BytesEnd::new("kernel")))?;

        // Write Git repository information if available
        if let Some(git_repo) = &self.config.git_repo {
            writer.write_event(Event::Start(BytesStart::new("git_repository")))?;

            // Write URL
            writer.write_event(Event::Start(BytesStart::new("url")))?;
            writer.write_event(Event::Text(BytesText::new(&git_repo.url)))?;
            writer.write_event(Event::End(BytesEnd::new("url")))?;

            // Write host
            writer.write_event(Event::Start(BytesStart::new("host")))?;
            let host_name = match &git_repo.host {
                GitHost::GitHub => "github.com",
                GitHost::GitLab => "gitlab.com",
                GitHost::Bitbucket => "bitbucket.org",
                GitHost::Other(name) => name,
            };
            writer.write_event(Event::Text(BytesText::new(host_name)))?;
            writer.write_event(Event::End(BytesEnd::new("host")))?;

            // Write owner
            writer.write_event(Event::Start(BytesStart::new("owner")))?;
            writer.write_event(Event::Text(BytesText::new(&git_repo.owner)))?;
            writer.write_event(Event::End(BytesEnd::new("owner")))?;

            // Write repository name
            writer.write_event(Event::Start(BytesStart::new("name")))?;
            writer.write_event(Event::Text(BytesText::new(&git_repo.name)))?;
            writer.write_event(Event::End(BytesEnd::new("name")))?;

            writer.write_event(Event::End(BytesEnd::new("git_repository")))?;
        }

        writer.write_event(Event::End(BytesEnd::new("system_info")))?;

        Ok(())
    }

    fn write_directory<W: Write>(
        &self,
        dir: &DirectoryNode,
        writer: &mut quick_xml::Writer<W>,
    ) -> io::Result<()> {
        let mut start_tag = BytesStart::new("directory");
        start_tag.push_attribute(("name", dir.name.as_str()));
        start_tag.push_attribute(("path", dir.path.to_string_lossy().as_ref()));
        writer.write_event(Event::Start(start_tag))?;

        // Write metadata only if enabled
        if self.config.include_metadata {
            self.write_metadata(&dir.metadata, writer)?;
        }

        // Write contents
        writer.write_event(Event::Start(BytesStart::new("contents")))?;

        for node in &dir.contents {
            match node {
                Node::Directory(dir_node) => self.write_directory(dir_node, writer)?,
                Node::File(file_node) => self.write_file(file_node, writer)?,
                Node::Binary(bin_node) => self.write_binary(bin_node, writer)?,
                Node::Symlink(sym_node) => self.write_symlink(sym_node, writer)?,
            }
        }

        writer.write_event(Event::End(BytesEnd::new("contents")))?;
        writer.write_event(Event::End(BytesEnd::new("directory")))?;

        Ok(())
    }

    fn write_file<W: Write>(
        &self,
        file: &FileNode,
        writer: &mut quick_xml::Writer<W>,
    ) -> io::Result<()> {
        let mut start_tag = BytesStart::new("file");
        start_tag.push_attribute(("name", file.name.as_str()));
        start_tag.push_attribute(("path", file.path.to_string_lossy().as_ref()));
        writer.write_event(Event::Start(start_tag))?;

        // Write metadata only if enabled
        if self.config.include_metadata {
            self.write_metadata(&file.metadata, writer)?;
        }

        // Write content
        writer.write_event(Event::Start(BytesStart::new("content")))?;
        if let Some(content) = &file.content {
            // Use CDATA section to preserve formatting and avoid XML parsing issues
            writer.write_event(Event::CData(BytesCData::new(content)))?;
        }
        writer.write_event(Event::End(BytesEnd::new("content")))?;

        writer.write_event(Event::End(BytesEnd::new("file")))?;

        Ok(())
    }

    fn write_binary<W: Write>(
        &self,
        binary: &BinaryNode,
        writer: &mut quick_xml::Writer<W>,
    ) -> io::Result<()> {
        let mut start_tag = BytesStart::new("binary");
        start_tag.push_attribute(("name", binary.name.as_str()));
        start_tag.push_attribute(("path", binary.path.to_string_lossy().as_ref()));
        writer.write_event(Event::Start(start_tag))?;

        // Write metadata only if enabled
        if self.config.include_metadata {
            self.write_metadata(&binary.metadata, writer)?;
        }

        writer.write_event(Event::End(BytesEnd::new("binary")))?;

        Ok(())
    }

    fn write_symlink<W: Write>(
        &self,
        symlink: &SymlinkNode,
        writer: &mut quick_xml::Writer<W>,
    ) -> io::Result<()> {
        let mut start_tag = BytesStart::new("symlink");
        start_tag.push_attribute(("name", symlink.name.as_str()));
        start_tag.push_attribute(("path", symlink.path.to_string_lossy().as_ref()));
        writer.write_event(Event::Start(start_tag))?;

        // Write metadata only if enabled
        if self.config.include_metadata {
            self.write_metadata(&symlink.metadata, writer)?;
        }

        // Write target
        writer.write_event(Event::Start(BytesStart::new("target")))?;
        writer.write_event(Event::Text(BytesText::new(&symlink.target)))?;
        writer.write_event(Event::End(BytesEnd::new("target")))?;

        writer.write_event(Event::End(BytesEnd::new("symlink")))?;

        Ok(())
    }

    fn write_overview<W: Write>(
        &self,
        root_node: &DirectoryNode,
        writer: &mut quick_xml::Writer<W>,
    ) -> io::Result<()> {
        writer.write_event(Event::Start(BytesStart::new("overview")))?;

        // Recursively write the directory structure with only names
        Self::write_node_overview(root_node, writer)?;

        writer.write_event(Event::End(BytesEnd::new("overview")))?;

        Ok(())
    }

    fn write_node_overview<W: Write>(
        dir: &DirectoryNode,
        writer: &mut quick_xml::Writer<W>,
    ) -> io::Result<()> {
        // Create a directory element with only the name
        let mut start_tag = BytesStart::new("directory");
        start_tag.push_attribute(("name", dir.name.as_str()));
        writer.write_event(Event::Start(start_tag))?;

        // Write child elements (files and directories)
        for node in &dir.contents {
            match node {
                Node::Directory(dir_node) => {
                    Self::write_node_overview(dir_node, writer)?;
                }
                Node::File(file_node) => {
                    let mut file_tag = BytesStart::new("file");
                    file_tag.push_attribute(("name", file_node.name.as_str()));
                    writer.write_event(Event::Empty(file_tag))?;
                }
                Node::Binary(bin_node) => {
                    let mut bin_tag = BytesStart::new("file");
                    bin_tag.push_attribute(("name", bin_node.name.as_str()));
                    writer.write_event(Event::Empty(bin_tag))?;
                }
                Node::Symlink(symlink_node) => {
                    let mut link_tag = BytesStart::new("symlink");
                    link_tag.push_attribute(("name", symlink_node.name.as_str()));
                    writer.write_event(Event::Empty(link_tag))?;
                }
            }
        }

        writer.write_event(Event::End(BytesEnd::new("directory")))?;

        Ok(())
    }

    fn write_metadata<W: Write>(
        &self,
        metadata: &Metadata,
        writer: &mut quick_xml::Writer<W>,
    ) -> io::Result<()> {
        writer.write_event(Event::Start(BytesStart::new("metadata")))?;

        // Write size
        writer.write_event(Event::Start(BytesStart::new("size")))?;
        writer.write_event(Event::Text(BytesText::new(&metadata.size.to_string())))?;
        writer.write_event(Event::End(BytesEnd::new("size")))?;

        // Write modified time
        writer.write_event(Event::Start(BytesStart::new("modified")))?;
        let modified = chrono::DateTime::<chrono::Local>::from(metadata.modified).to_rfc3339();
        writer.write_event(Event::Text(BytesText::new(&modified)))?;
        writer.write_event(Event::End(BytesEnd::new("modified")))?;

        // Write permissions
        writer.write_event(Event::Start(BytesStart::new("permissions")))?;
        writer.write_event(Event::Text(BytesText::new(&metadata.permissions)))?;
        writer.write_event(Event::End(BytesEnd::new("permissions")))?;

        writer.write_event(Event::End(BytesEnd::new("metadata")))?;

        Ok(())
    }
}

impl Writer for XmlWriter {
    fn write(&self, root_node: &DirectoryNode) -> io::Result<()> {
        let file = File::create(&self.config.output_file)?;
        let writer = BufWriter::new(file);
        let mut xml_writer = quick_xml::Writer::new_with_indent(writer, b' ', 2);

        // Write XML declaration
        xml_writer.write_event(Event::Decl(BytesDecl::new("1.0", Some("UTF-8"), None)))?;

        // Start directory_scan element with timestamp
        let mut start_tag = BytesStart::new("directory_scan");
        let timestamp = Local::now().to_rfc3339();
        start_tag.push_attribute(("timestamp", timestamp.as_str()));
        xml_writer.write_event(Event::Start(start_tag))?;

        // Write system info
        self.write_system_info(&mut xml_writer)?;

        // Write repository structure summary
        self.write_overview(root_node, &mut xml_writer)?;

        // Write directory structure
        self.write_directory(root_node, &mut xml_writer)?;

        // End directory_scan element
        xml_writer.write_event(Event::End(BytesEnd::new("directory_scan")))?;

        Ok(())
    }
}

/// Simple text writer for directory contents
struct TxtWriter {
    config: Config,
    root_node_path: PathBuf,
}

impl TxtWriter {
    pub fn new(config: Config) -> Self {
        Self {
            config,
            root_node_path: Default::default(),
        }
    }

    fn write_system_info<W: Write>(&self, writer: &mut W) -> io::Result<()> {
        let hostname = hostname::get()
            .map(|h| h.to_string_lossy().to_string())
            .unwrap_or_else(|_| "unknown".to_string());

        writeln!(writer, "Hostname: {}", hostname)?;
        writeln!(writer, "OS: {}", std::env::consts::OS)?;
        writeln!(writer, "Kernel: {}", std::env::consts::FAMILY)?;
        Ok(())
    }

    fn write_repo_info<W: Write>(&self, writer: &mut W) -> io::Result<()> {
        if let Some(git_repo) = &self.config.git_repo {
            writeln!(writer, "URL: {}", git_repo.url)?;
            let host_name = match &git_repo.host {
                GitHost::GitHub => "github.com",
                GitHost::GitLab => "gitlab.com",
                GitHost::Bitbucket => "bitbucket.org",
                GitHost::Other(name) => name,
            };
            writeln!(writer, "Host: {}", host_name)?;
            writeln!(writer, "Owner: {}", git_repo.owner)?;
            writeln!(writer, "Repository: {}", git_repo.name)?;
        }
        Ok(())
    }

    fn write_directory<W: Write>(&self, dir: &DirectoryNode, writer: &mut W) -> io::Result<()> {
        for node in &dir.contents {
            match node {
                Node::Directory(dir_node) => self.write_directory(dir_node, writer)?,
                Node::File(file_node) => self.write_file(file_node, writer)?,
                Node::Binary(bin_node) => self.write_binary(bin_node, writer)?,
                Node::Symlink(sym_node) => self.write_symlink(sym_node, writer)?,
            }
        }
        Ok(())
    }

    fn write_file<W: Write>(&self, file: &FileNode, writer: &mut W) -> io::Result<()> {
        if let Some(content) = &file.content {
            let filename = file
                .path
                .strip_prefix(&self.root_node_path)
                .expect("file path should start with root_dir");
            let extension = filename
                .extension()
                .map(|v| v.to_string_lossy())
                .unwrap_or_default();

            writeln!(writer, "\n================================================")?;
            writeln!(writer, "{}", filename.display())?;
            writeln!(writer, "================================================\n")?;

            if self.config.include_metadata {
                self.write_metadata(&file.metadata, writer)?;
            }
            writeln!(writer, "```{}", extension)?;
            writeln!(writer, "{}", content)?;
            writeln!(writer, "```")?;
        }
        Ok(())
    }

    fn write_binary<W: Write>(&self, binary: &BinaryNode, writer: &mut W) -> io::Result<()> {
        if self.config.include_metadata {
            self.write_metadata(&binary.metadata, writer)?;
        }
        Ok(())
    }

    fn write_symlink<W: Write>(&self, symlink: &SymlinkNode, writer: &mut W) -> io::Result<()> {
        writeln!(
            writer,
            "[S] {} -> {}",
            symlink.path.display(),
            symlink.target
        )?;
        if self.config.include_metadata {
            self.write_metadata(&symlink.metadata, writer)?;
        }
        Ok(())
    }

    fn write_metadata<W: Write>(&self, metadata: &Metadata, writer: &mut W) -> io::Result<()> {
        writeln!(writer, "  Size: {}", metadata.size)?;
        writeln!(
            writer,
            "  Modified: {}",
            chrono::DateTime::<chrono::Local>::from(metadata.modified).to_rfc3339()
        )?;
        writeln!(writer, "  Permissions: {}", metadata.permissions)?;
        Ok(())
    }
}

impl Writer for TxtWriter {
    fn write(&self, root_node: &DirectoryNode) -> io::Result<()> {
        let file = File::create(&self.config.output_file)?;
        let mut writer = BufWriter::new(file);

        if self.config.include_metadata {
            // Write system info section
            writeln!(
                writer,
                "=================== SYSTEM INFO ==================="
            )?;
            self.write_system_info(&mut writer)?;
            writeln!(writer)?;
        }
        // Write repository info if available
        if self.config.git_repo.is_some() {
            writeln!(writer, "=================== REPOSITORY ===================")?;
            self.write_repo_info(&mut writer)?;
            writeln!(writer)?;
        }

        // Write directory structure
        writeln!(writer, "<codebase name=\"{}\">", root_node.name)?;
        self.write_directory(root_node, &mut writer)?;
        writeln!(writer, "</codebase>")?;

        Ok(())
    }
}

```

================================================
/home/user/projs/dumpfs/tests/clipboard_integration.rs
================================================

```rs
/*!
 * Integration test for clipboard functionality
 */

use std::env;
use std::fs::{self, File};
use std::io::Write;
use std::process::Command;

use tempfile::tempdir;

#[test]
#[ignore] // This test requires tmux to be running and is ignored by default
          // To run this test manually use: cargo test --test clipboard_integration -- --ignored
fn test_clip_flag() {
    // Skip if not in a tmux session
    if env::var("TMUX").is_err() {
        return;
    }

    // Create a temporary directory with some test files
    let temp_dir = tempdir().unwrap();
    let test_file = temp_dir.path().join("test.txt");
    let output_file = temp_dir.path().join("output.xml");

    // Write some content to the test file
    let mut file = File::create(&test_file).unwrap();
    writeln!(file, "Test content for clipboard integration").unwrap();

    // Build the project first to ensure binary is available
    assert!(Command::new("cargo")
        .args(["build"])
        .status()
        .unwrap()
        .success());

    // Run dumpfs with --clip flag
    // The command format is: dumpfs [OPTIONS] [DIRECTORY_PATH] [OUTPUT_FILE]
    let status = Command::new("cargo")
        .args([
            "run",
            "--",
            "--clip",
            &temp_dir.path().to_string_lossy(), // Directory path (positional)
            &output_file.to_string_lossy(),     // Output file (positional)
        ])
        .status()
        .unwrap();

    // Check that the command succeeded
    assert!(status.success());

    // Verify that the output file exists
    assert!(output_file.exists());

    // Get the content from the output file
    let xml_content = fs::read_to_string(&output_file).unwrap();

    // Get the content from the tmux clipboard
    let clipboard_output = Command::new("tmux").args(["show-buffer"]).output().unwrap();

    let clipboard_content = String::from_utf8_lossy(&clipboard_output.stdout);

    // Verify that the clipboard contains the XML content
    assert_eq!(xml_content, clipboard_content);
}

```

================================================
dumpfs/Cargo.toml
================================================

```toml
[package]
name = "dumpfs"
version = "0.1.0"
edition = "2021"
description = "Generate XML representation of directory contents for LLM context"
authors = ["kkharji"]
license = "MIT"

[dependencies]
clap = { version = "4.3", features = ["derive"] }
clap_complete = { version = "4.3", features = ["unstable-dynamic"] }
walkdir = "2.3"
git2 = "0.18"
url = "2.5"
regex = "1.10"
thiserror = "1.0"
quick-xml = "0.37.3"
rayon = "1.7"
indicatif = "0.17"
glob-match = "0.2"
chrono = "0.4"
once_cell = "1.18"
hostname = "0.4.0"
ignore = "0.4.21"
tabled = "0.18.0"
tokenizers = { version = "0.21.1",  features = ["http"] }
strum = { version = "0.27.1", features = ["derive"] }
reqwest = { version = "0.12.15", features = ["json", "blocking"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tiktoken-rs = "0.6.0"
dirs = "6.0.0"

[dev-dependencies]
tempfile = "3.8"
filetime = "0.2.15"

[profile.release]
lto = true
codegen-units = 1
panic = "abort"
strip = true

```

================================================
dumpfs/README.md
================================================

```md
# DumpFS: Directory Context Generator for LLMs

`dumpfs` is a command-line tool that generates an XML representation of directory contents, designed specifically for providing context to Large Language Models (LLMs) for coding tasks.

## Features

- Recursively scans directories and generates structured XML output
- Includes file content with CDATA wrapping
- Handles different file types (text, binary, symlinks)
- Provides file metadata (size, modification time, permissions)
- Supports pattern-based inclusion and exclusion of files
- Respects `.gitignore` files for intelligent filtering
- Supports Git repository URLs (GitHub, GitLab, Bitbucket, and more)
- Automatically clones and manages repositories in a local cache
- Parallel processing for better performance
- Progress tracking with ETA and detailed file statistics
- Beautiful Unicode progress display with real-time file information
- Comprehensive summary of scanned content with LLM token estimation
- Intelligent caching of tokenized files for faster processing

## Installation

### From Source

```bash
git clone https://github.com/kkharji/dumpfs.git
cd dumpfs
cargo build --release
```

The binary will be available at `target/release/dumpfs`.

## Usage

```
dumpfs [DIRECTORY_PATH|GIT_URL] [OUTPUT_FILE] [OPTIONS]

OPTIONS:
    --ignore-patterns <pattern1,pattern2,...>    Comma-separated list of patterns to ignore
    --include-patterns <pattern1,pattern2,...>   Comma-separated list of patterns to include
    --threads <N>                                Number of threads to use for processing
    --respect-gitignore <BOOL>                   Whether to respect .gitignore files (default: true)
    --gitignore-path <PATH>                      Path to custom .gitignore file
    --model <MODEL>                              LLM model to use for tokenization
    --generate <SHELL>                           Generate shell completions (bash, zsh, fish, etc.)
    --clean-cache <DAYS>                         Clean Git repository cache older than DAYS (0 for all)
    --clip                                       Copy output to system clipboard
```

### Supported Models

The `--model` option enables accurate token counting and caching. Supported models include:

**OpenAI Models:**
- `gpt-4` - GPT-4 (8K context window)
- `gpt-4-turbo` - GPT-4 Turbo (128K context window)
- `gpt4o` - GPT-4o (8K context window)

**Anthropic Models:**
- `sonnet-3.5` - Claude 3.5 Sonnet (200K context window)
- `sonnet-3.7` - Claude 3.7 Sonnet (200K context window)

**HuggingFace Models:**
- `llama-2-7b` - Llama 2 7B (4K context window)
- `llama-3-8b` - Llama 3 8B (8K context window)
- `mistral-small` - Mistral Small (32K context window)
- `mistral-small-24b` - Mistral Small 24B (128K context window)
- `mistral-large` - Mistral Large (128K context window)
- `pixtral-12b` - Pixtral 12B (128K context window)

When a model is specified, `dumpfs` provides exact token counts instead of estimates and caches results for faster processing on subsequent runs.

When running the command, you'll see a beautiful progress display showing:

- Real-time progress with an animated spinner
- Current file being processed
- Progress bar showing completion percentage
- Processing speed (files per second)
- Estimated time remaining

After completion, you'll get a comprehensive summary showing file statistics and token estimation for LLM usage.

### Shell Completion

dumpfs supports shell completion for Bash, Fish, Zsh, Elvish, and PowerShell. To generate completion scripts, use:

```bash
# For bash
dumpfs --generate bash > ~/.local/share/bash-completion/completions/dumpfs

# For zsh
dumpfs --generate zsh > ~/.zfunc/_dumpfs

# For fish
dumpfs --generate fish > ~/.config/fish/completions/dumpfs.fish

# For PowerShell
dumpfs --generate powershell > _dumpfs.ps1
```

For Zsh, make sure to add `~/.zfunc` to your `fpath` in your `.zshrc`:

```zsh
fpath=( ~/.zfunc $fpath )
```

Shell completion provides auto-completion for commands, options, and even supported model types for the `--model` option.


### Examples

```bash
# Process current directory
dumpfs

# Process specific directory with custom output file
dumpfs /path/to/project project_context.xml

# Process a GitHub repository
dumpfs https://github.com/username/repo

# Process a GitLab repository
dumpfs https://gitlab.com/username/repo

# Process a repository via SSH URL
dumpfs git@github.com:username/repo.git

# Ignore specific patterns
dumpfs --ignore-patterns "*.log,*.tmp,*.bak"

# Include only specific patterns
dumpfs --include-patterns "*.rs,*.toml,*.md"

# Use 8 threads for processing
dumpfs --threads 8

# Disable .gitignore respect
dumpfs --respect-gitignore false

# Use custom gitignore file
dumpfs --gitignore-path /path/to/custom/gitignore

# Use specific model for token counting with caching
dumpfs --model gpt4o

# Copy the output XML to system clipboard
dumpfs --clip

# Clean Git repository cache older than 30 days
dumpfs --clean-cache 30

# Clean all Git repository cache
dumpfs --clean-cache 0
```

## Git Repository Support

`dumpfs` supports generating context directly from Git repositories by specifying a repository URL. The tool will clone the repository to a local cache directory (`~/.cache/dumpfs/`) and process it like a local directory.

### Supported Repository URL Formats

- GitHub: `https://github.com/username/repo` or `git@github.com:username/repo.git`
- GitLab: `https://gitlab.com/username/repo` or `git@gitlab.com:username/repo.git`
- Bitbucket: `https://bitbucket.org/username/repo` or `git@bitbucket.org:username/repo.git`
- Other Git hosts: Any valid HTTP/HTTPS or SSH Git URL

### Repository Caching

Repositories are stored in the following locations, organized by hosting platform:

- GitHub: `~/.cache/dumpfs/github/username/repo`
- GitLab: `~/.cache/dumpfs/gitlab/username/repo`
- Bitbucket: `~/.cache/dumpfs/bitbucket/username/repo`
- Other: `~/.cache/dumpfs/git/hostname/username/repo`

When processing a repository that's already in the cache, `dumpfs` will automatically update it with the latest changes from the remote.

You can clean up old cached repositories using the `--clean-cache` option followed by the age in days. For example, `--clean-cache 30` will remove repositories that haven't been accessed in the last 30 days. Using `--clean-cache 0` will clean all cached repositories.

## GitIgnore Support

By default, `dumpfs` respects `.gitignore` files in the project directory. This means that files and directories that would be ignored by Git are also ignored by `dumpfs`. This is useful for excluding build artifacts, dependencies, and other files that are not relevant to the codebase.

You can disable this behavior with the `--respect-gitignore false` option, or specify a custom gitignore file with the `--gitignore-path` option.

## Output Format

The tool generates an XML file with the following structure:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<directory_scan timestamp="2025-03-26T12:34:56+00:00">
  <system_info>
    <hostname>your-hostname</hostname>
    <os>linux</os>
    <kernel>unix</kernel>
    <!-- Git repository information (if applicable) -->
    <git_repository>
      <url>https://github.com/username/repo</url>
      <host>github.com</host>
      <owner>username</owner>
      <name>repo</name>
    </git_repository>
  </system_info>
  <directory name="project" path="project">
    <metadata>
      <size>4096</size>
      <modified>2025-03-26T12:34:56+00:00</modified>
      <permissions>755</permissions>
    </metadata>
    <contents>
      <file name="example.rs" path="project/example.rs">
        <metadata>
          <size>1024</size>
          <modified>2025-03-26T12:34:56+00:00</modified>
          <permissions>644</permissions>
        </metadata>
        <content><![CDATA[fn main() {
    println!("Hello, world!");
}]]></content>
      </file>
      <!-- More files and directories -->
    </contents>
  </directory>
</directory_scan>
```

## Example Output

When running `dumpfs`, you'll initially see scanning messages and a progress bar. After completion, the progress information is automatically cleared, and you'll see a comprehensive summary with the processed files followed by extraction statistics:

```
ðŸ“‹  PROCESSED FILES
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ File Path      â”‚ Lines â”‚ Est. Tokens â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ src/scanner.rs â”‚ 461   â”‚ 4.1K        â”‚
â”‚ src/tests.rs   â”‚ 330   â”‚ 2.9K        â”‚
â”‚ src/report.rs  â”‚ 272   â”‚ 2.1K        â”‚
â”‚ src/writer.rs  â”‚ 202   â”‚ 2.0K        â”‚
â”‚ README.md      â”‚ 170   â”‚ 1.5K        â”‚
â”‚ src/utils.rs   â”‚ 209   â”‚ 1.2K        â”‚
â”‚ src/config.rs  â”‚ 119   â”‚ 928         â”‚
â”‚ src/main.rs    â”‚ 113   â”‚ 870         â”‚
â”‚ src/types.rs   â”‚ 95    â”‚ 538         â”‚
â”‚ src/lib.rs     â”‚ 27    â”‚ 188         â”‚
â”‚ Cargo.toml     â”‚ 29    â”‚ 135         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

âœ…  EXTRACTION COMPLETE
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Metric             â”‚ Value                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“‚ Output File     â”‚ .dumpfs.context.xml         â”‚
â”‚ â±ï¸ Process Time    â”‚ 10.8125ms                   â”‚
â”‚ ðŸ“„ Files Processed â”‚ 12                          â”‚
â”‚ ðŸ“ Total Lines     â”‚ 3.0K                        â”‚
â”‚ ðŸ“¦ LLM Tokens      â”‚ 21.2K tokens (counted)      â”‚
â”‚ ðŸ”„ Cache Hit Rate  â”‚ 100.0% (12 hits / 12 total) â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

The output provides:
- A detailed breakdown of each file with line counts and token counts
- File paths displayed relative to the project root
- Human-readable numbers with K suffixes for large values
- Total processing time with millisecond precision
- Total number of files processed
- Total line count
- Exact token usage for LLM context (when using a model)
- Cache hit rate showing tokenization efficiency

This information is particularly valuable when preparing context for LLMs, as it helps you understand the size and composition of the context you're providing.

## Token Caching

When using the `--model` option, dumpfs implements intelligent caching of tokenized content:

- Only tokenizes files that haven't been processed before or have changed
- Persists cache between runs in `~/.cache/dumpfs/[project_path].token_cache.json`
- Automatically cleans up old cache entries (older than 7 days)
- Reports cache hit rate in the output summary

This caching mechanism significantly improves performance when running the tool multiple times on the same codebase, especially with API-based tokenizers like those from OpenAI or Anthropic.

**First run with caching:**
```
ðŸ“¦ LLM Tokens      â”‚ 21.2K tokens (counted)      â”‚
ðŸ”„ Cache Hit Rate  â”‚ 0.0% (0 hits / 12 total)    â”‚
```

**Subsequent runs:**
```
ðŸ“¦ LLM Tokens      â”‚ 21.2K tokens (counted)      â”‚
ðŸ”„ Cache Hit Rate  â”‚ 100.0% (12 hits / 12 total) â”‚
```

Tokenization is often the most time-consuming part of the process, especially when using remote API-based tokenizers, so this caching mechanism can dramatically improve performance for repeated scans.

## License

MIT

```
</codebase>
